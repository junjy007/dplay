{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from dqn_utils.env_wrapper_NES import get_contra_env\n",
    "from dqn_evaluate_contra import MultiDQNPolicy, evaluate_single_policy, \\\n",
    "    evaluate_multimodel_by_start_image\n",
    "from dqn_model import DQN\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_history_len = 4\n",
    "demo0 = {'models': [('checkpoints_contra_s00', 'Contra_stage0_0.nss', 2000000),\n",
    "                    ('checkpoints_contra_s01', 'Contra_stage0_1.nss', 1750000),\n",
    "                    ('checkpoints_contra_s02', 'Contra_stage0_2.nss', 2000000),\n",
    "                    ('checkpoints_contra_s03', 'Contra_stage0_3.nss', 7350000),\n",
    "                   ]}\n",
    "resdir = os.path.abspath('dqn_utils/nes_env/res/')\n",
    "\n",
    "state_fnames, bestmodel_fnames, log_fnames = [], [], []\n",
    "\n",
    "for cpdir, nssfname, bestmod_id in demo0['models']:\n",
    "    cpdir_ = os.path.abspath(cpdir)\n",
    "    log_fname = os.path.join(cpdir_, 'latest.json')\n",
    "    log_fnames.append(log_fname)\n",
    "    \n",
    "    state_fname = os.path.join(resdir, nssfname)\n",
    "    state_fnames.append(state_fname)\n",
    "    \n",
    "    bestmodel_fname = os.path.join(cpdir_, 'cp{}.torchmodel'.format(bestmod_id))\n",
    "    bestmodel_fnames.append(bestmodel_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resdir = os.path.abspath('dqn_utils/nes_env/res/')\n",
    "trn_start_nss = [os.path.join(resdir, 'Contra_stage0_{}.nss'.format(i)) for i in range(4)]\n",
    "modelfiles = [os.path.abspath('checkpoints_contra_s0{}/cp{}.torchmodel'.format(i,ci))\n",
    "              for i, ci in zip(range(4), [2000000, 1750000, 2010000, 2200000])]\n",
    "\n",
    "settings = [{'epsilon': 0.02, 'agamma':0.8}, {'epsilon': 0.02, 'agamma':0.8},\n",
    "            {'epsilon': 0.02, 'agamma':0.8}, {'epsilon': 0.3, 'agamma':0.8}]\n",
    "\n",
    "models = {\n",
    "    'trained_models': modelfiles,\n",
    "    'training_start_stages': trn_start_nss,\n",
    "    'eval_settings': settings\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run dqn_evaluate_contra.py\n",
    "acts = evaluate_multimodel_by_start_image('tmp/play1', models, start_mini_stage=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run dqn_evaluate_contra.py\n",
    "nssfiles = [os.path.abspath('tmp/play1/s{}.nss'.format(i)) for i in range(4)]\n",
    "show_game_play(nssfiles, acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_id in range(2000000, 2500000, 100000):\n",
    "    print model_id\n",
    "    for RANDSEED in range(3):\n",
    "        rng = np.random.RandomState(RANDSEED)\n",
    "        rec = evaluate_single_policy(state_fnames[3], \n",
    "            '/home/junli/projects/dplay/RUNS/t0821_nature_dqn4/'\n",
    "            'checkpoints_contra_s03/cp{}.torchmodel'.format(model_id), \n",
    "            rng, epsilon=0.3, agamma=0.8)\n",
    "        #avs = np.stack(rec.predicted_action_values).squeeze()\n",
    "        #plt.plot(avs.max(axis=1))\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get starting scenario image for each trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = get_contra_env()\n",
    "img_h, img_w, img_c = env.observation_space.shape\n",
    "input_arg = frame_history_len * img_c  # in_channels = #.frame-history * channels per frame\n",
    "num_actions = env.action_space.n\n",
    "def Contra_DQN():\n",
    "    return DQN(input_arg, num_actions, img_h, img_w).type(FloatTensor)\n",
    "del env\n",
    "mp = MultiDQNPolicy(Contra_DQN, get_contra_env, bestmodel_fnames, state_fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get scene images\n",
    "scene_images = []\n",
    "for nss_fname in state_fnames:\n",
    "    env = get_contra_env(nss_fname)\n",
    "    env.reset()\n",
    "    env.step(0)\n",
    "    scene_images.append(env.frame())\n",
    "    del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get models\n",
    "dqns = []\n",
    "for mfname in bestmodel_fnames:\n",
    "    Q = DQN(input_arg, num_actions, img_h, img_w).type(FloatTensor)\n",
    "    Q.load(bestmodel_fnames[mod_id])\n",
    "    dqns.append(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check performance to choose the checkpoint to load\n",
    "mod_id = 1\n",
    "if mod_id != 3: # the last one's log has been corrupted.\n",
    "    with open(log_fnames[mod_id], 'r') as f:\n",
    "        logd = json.load(f)\n",
    "        mean_rew = logd['mean_episode_reward']\n",
    "else:\n",
    "    with open(log_fnames[mod_id], 'r') as f:\n",
    "        txt = f.read()\n",
    "    mean_rew = np.fromstring(txt[60:], dtype=float, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(mean_rew)\n",
    "plt.ylim([-10,20])\n",
    "plt.show()\n",
    "print np.argmax(mean_rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# quick dirty testing ...\n",
    "from dqn_utils.replaybuffer import ReplayBuffer\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from dqn_utils.evaluation import EvaluationRecord\n",
    "from itertools import count\n",
    "\n",
    "env = get_contra_env()\n",
    "\n",
    "replay_buffer_size = 250000\n",
    "frame_history_len = 4\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "\n",
    "def select_greedy_action(model, s):\n",
    "    s_ = torch.from_numpy(s).type(FloatTensor).unsqueeze(0) / 255.0\n",
    "    # unsqueeze(0) => to make the observation a one-sample batch\n",
    "    predicted_action_values = model(Variable(s_, volatile=True)).data  # type: torch.FloatTensor\n",
    "    greedy_action = predicted_action_values.max(dim=1)[1].cpu()\n",
    "    # the 2nd return val of max is the index of the max (argmax) in each row (since\n",
    "    # we have specified dim=1 in the function call)\n",
    "    return greedy_action, predicted_action_values\n",
    "\n",
    "img_h, img_w, img_c = env.observation_space.shape\n",
    "input_arg = frame_history_len * img_c  # in_channels = #.frame-history * channels per frame\n",
    "num_actions = env.action_space.n\n",
    "    \n",
    "#Q = DQN(input_arg, num_actions, img_h, img_w).type(FloatTensor)\n",
    "#Q.load(bestmodel_fnames[mod_id])\n",
    "rec = EvaluationRecord(\n",
    "        observations=[],\n",
    "        hidden_features=None, #collector_hook.data,\n",
    "        predicted_action_values=[],\n",
    "        actions=[])\n",
    "        \n",
    "\n",
    "scene_image_diff = []\n",
    "mini_stage = 0\n",
    "if mini_stage == 0:\n",
    "    next_stage_state_file = state_fnames[0]\n",
    "else:\n",
    "    next_stage_state_file = os.path.abspath(\n",
    "        'tmp/play/mini_stage_{}.nss'.format(mini_stage))\n",
    "    mp.set_current_model(mini_stage)\n",
    "    \n",
    "while mini_stage < 2:\n",
    "    print \"Now do {}\".format(next_stage_state_file)\n",
    "    env.load_state(next_stage_state_file)\n",
    "    mini_stage_done = False\n",
    "    \n",
    "    while not mini_stage_done:\n",
    "        # env = get_contra_env(next_stage_state_file)\n",
    "        replay_buffer = ReplayBuffer(replay_buffer_size, frame_history_len)\n",
    "        last_obs = env.reset()\n",
    "        for t in count():\n",
    "            buf_idx = replay_buffer.store_frame(last_obs)\n",
    "            recent_observations = replay_buffer.encode_recent_observation()\n",
    "\n",
    "            did_change_model, sc_ = mp.change_model_when_ready(env, t)\n",
    "            scene_image_diff.append(sc_)\n",
    "            Q = mp.get_current_model()\n",
    "            if did_change_model:\n",
    "                # save\n",
    "                mini_stage_done = True\n",
    "                print \"Mini stage {} cleared\".format(mini_stage)\n",
    "                mini_stage += 1\n",
    "                next_stage_state_file = 'tmp/play/mini_stage_{}.nss'.format(mini_stage)\n",
    "                env.save_state(next_stage_state_file)\n",
    "                break\n",
    "            action, action_values = select_greedy_action(Q, recent_observations)\n",
    "            if t<10:\n",
    "                print action_values, action \n",
    "            action = action[0, 0]\n",
    "            action_values = action_values.cpu().numpy()\n",
    "            rec.observations.append(recent_observations)\n",
    "            rec.predicted_action_values.append(action_values)\n",
    "            rec.actions.append(action)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            replay_buffer.store_effect(buf_idx, action, reward, done)\n",
    "            #rr += reward\n",
    "            last_obs = obs\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        \n",
    "# draw values\n",
    "avs = np.vstack(rec.predicted_action_values)\n",
    "plt.clf()\n",
    "#for i in range(19):\n",
    "#    plt.plot(avs[:,i])\n",
    "plt.plot(avs.max(axis=1))\n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.plot(scs_)\n",
    "plt.show()\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0 62.9148065476\n",
    "Model id 140567545210704\n",
    "1 62.5227864583\n",
    "Model id 140567545210704\n",
    "2 60.5958891369\n",
    "Model id 140567545210704\n",
    "3 63.6144903274\n",
    "Model id 140567545210704\n",
    "\n",
    "\n",
    "plt.plot(scs_, 'b.-')\n",
    "plt.ylim([10,30])\n",
    "plt.xlim([100,450])\n",
    "plt.grid('on')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avs = np.vstack(rec.predicted_action_values)\n",
    "plt.clf()\n",
    "#for i in range(19):\n",
    "#    plt.plot(avs[:,i])\n",
    "plt.plot(avs.max(axis=1))\n",
    "plt.xlim([0,100])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Meta Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dqn_utils.replaybuffer import ReplayBuffer\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from dqn_utils.evaluation import EvaluationRecord\n",
    "\n",
    "\n",
    "replay_buffer_size = 250000\n",
    "frame_history_len = 4\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "\n",
    "\n",
    "def evaluate(env, saved_models, maxsteps, meta_policy):\n",
    "    \"\"\"\n",
    "    :param saved_models: list of saved model file names\n",
    "    :param meta_policy: how to choose acting policy\n",
    "    \"\"\"\n",
    "    # - greedy policy\n",
    "    def select_greedy_action(model, s):\n",
    "        s_ = torch.from_numpy(s).type(FloatTensor).unsqueeze(0) / 255.0\n",
    "        # unsqueeze(0) => to make the observation a one-sample batch\n",
    "        predicted_action_values = model(Variable(s_, volatile=True)).data  # type: torch.FloatTensor\n",
    "        greedy_action = predicted_action_values.max(dim=1)[1].cpu()\n",
    "        # the 2nd return val of max is the index of the max (argmax) in each row (since\n",
    "        # we have specified dim=1 in the function call)\n",
    "        return greedy_action, predicted_action_values\n",
    "    \n",
    "    img_h, img_w, img_c = env.observation_space.shape\n",
    "    input_arg = frame_history_len * img_c  # in_channels = #.frame-history * channels per frame\n",
    "    num_actions = env.action_space.n\n",
    "    replay_buffer = ReplayBuffer(replay_buffer_size, frame_history_len)\n",
    "    \n",
    "    recs = []\n",
    "    Qs = []\n",
    "    for fn in saved_models:\n",
    "        Q = DQN(input_arg, num_actions, img_h, img_w).type(FloatTensor)\n",
    "        Q.load(fn)\n",
    "        Qs.append(Q)\n",
    "        rec = EvaluationRecord(\n",
    "                observations=[],\n",
    "                hidden_features=None, #collector_hook.data,\n",
    "                predicted_action_values=[],\n",
    "                actions=[])\n",
    "        recs.append(rec)\n",
    "        \n",
    "    last_obs = env.reset()\n",
    "    for t in range(maxsteps):\n",
    "        replay_buffer.store_frame(last_obs)\n",
    "        recent_observations = replay_buffer.encode_recent_observation()\n",
    "        \n",
    "        action_candidates = []\n",
    "        action_value_predictions = []\n",
    "        for Q, rec in zip(Qs, recs):\n",
    "            action, action_values = select_greedy_action(Q, recent_observations)\n",
    "            action = action[0, 0]\n",
    "            action_candidates.append(action)\n",
    "            action_values = action_values.cpu().numpy()\n",
    "            action_value_predictions.append(action_values)\n",
    "            rec.observations.append(recent_observations)\n",
    "            rec.predicted_action_values.append(action_values)\n",
    "            rec.actions.append(action)\n",
    "            \n",
    "        act_model_id = meta_policy(t, action_value_predictions)\n",
    "        action = action_candidates[act_model_id]\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        #rr += reward\n",
    "        last_obs = obs\n",
    "        if done:\n",
    "            break\n",
    "    return recs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = get_contra_env(state_fnames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mp = MetaPolicy()\n",
    "recs = evaluate(env, bestmodel_fnames, maxsteps=5000, meta_policy=mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot recorded predicted actition values during the game\n",
    "max_expect_rewards = []\n",
    "for rec_ in recs:\n",
    "    avs_ = np.vstack(rec_.predicted_action_values)\n",
    "    max_expect_rewards.append(avs_.max(axis=1))\n",
    "plt.clf()\n",
    "axhandles = []\n",
    "labels = []\n",
    "for i_, mr_ in enumerate(max_expect_rewards):\n",
    "    h_, = plt.plot(mr_)\n",
    "    axhandles.append(h_)\n",
    "    labels.append(\"model-{}\".format(i_))\n",
    "plt.legend(axhandles, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(replay_buffer_size, frame_history_len)\n",
    "\n",
    "last_obs = env.reset()\n",
    "replay_buffer.store_frame(last_obs)\n",
    "for i in range(10):\n",
    "    last_obs, _, _, _ = env.step(0)\n",
    "last_obs, _, _, _ = env.step(0)\n",
    "rr= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t in range(10000):\n",
    "    replay_buffer.store_frame(last_obs)\n",
    "    recent_observations = replay_buffer.encode_recent_observation()\n",
    "    action, action_values = select_greedy_action(Q, recent_observations)\n",
    "    action = action[0, 0]\n",
    "    action_values = action_values.cpu().numpy()\n",
    "    rec.observations.append(recent_observations)\n",
    "    rec.predicted_action_values.append(action_values)\n",
    "    rec.actions.append(action)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    rr += reward\n",
    "    last_obs = obs\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avs = np.vstack(rec.predicted_action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "#for i in range(19):\n",
    "#    plt.plot(avs[:,i])\n",
    "plt.plot(avs.max(axis=1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rec = dqn_evaluate(env=env, q_func=DQN, trained_model_fname=bestmodel_fname, max_eval_steps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test if we can repeat our experiment\n",
    "SUCCEED \n",
    "DEBUGGED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "%run dqn_evaluate_contra.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nssfile = os.path.abspath('dqn_utils/nes_env/res/Contra_stage0_2.nss')\n",
    "modelfile = os.path.abspath('checkpoints_contra_s02/cp2000000.torchmodel')\n",
    "RANDSEED=0\n",
    "rng = np.random.RandomState(RANDSEED)\n",
    "obs_rec_1, act_rec_1, actval_rec_1, frm_rec_1, rb1 = \\\n",
    "    test_repeat_model_eval(nssfile, modelfile, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(RANDSEED)\n",
    "obs_rec_2, act_rec_2, actval_rec_2, frm_rec_2, rb2 = \\\n",
    "    test_repeat_model_eval(nssfile, modelfile, rng, {'obs': obs_rec_1})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_1 = np.stack(obs_rec_1)\n",
    "act_1 = np.stack(act_rec_1)\n",
    "val_1 = np.stack(actval_rec_1)\n",
    "frm_1 = np.stack(frm_rec_1)\n",
    "obs_2 = np.stack(obs_rec_2)\n",
    "act_2 = np.stack(act_rec_2)\n",
    "val_2 = np.stack(actval_rec_2)\n",
    "frm_2 = np.stack(frm_rec_2)\n",
    "n1 = obs_1.shape[0]\n",
    "n2 = obs_2.shape[0]\n",
    "n = min(n1, n2)\n",
    "print \"Observation numbers: test-1: {}; test-2: {}\".format(n1, n2)\n",
    "print \"Observation shape {}\".format(obs_1.shape[1:])\n",
    "print \"Frame shape {}\".format(frm_1.shape[1:])\n",
    "print \"Act value shape {}\".format(val_1.shape[1:])\n",
    "\n",
    "def fn_diff(x1, x2):\n",
    "    return np.abs(x1[:n] - x2[:n]).reshape(n, -1).max(axis=1)\n",
    "    \n",
    "obs_d = fn_diff(obs_1, obs_2)\n",
    "act_d = fn_diff(act_1, act_2)\n",
    "val_d = fn_diff(val_1, val_2)\n",
    "frm_d = fn_diff(frm_1, frm_2)\n",
    "\n",
    "def str_first_nonzero(x): # x must be 1d\n",
    "    i = np.nonzero(x)[0] \n",
    "    if i.size > 0:\n",
    "        i = i[0]\n",
    "        return \"[{}]:{:.2f}\".format(i, x[i])\n",
    "    return \"None\"\n",
    "\n",
    "print \"Compare observations: {}\".format(str_first_nonzero(obs_d))\n",
    "print \"Compare actions: {}\".format(str_first_nonzero(act_d))\n",
    "print \"Compare values: {}\".format(str_first_nonzero(val_d))\n",
    "print \"Compare frames: {}\".format(str_first_nonzero(frm_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the aboive cell show there is problems in the replay memory:\n",
    "\n",
    "Observation numbers: test-1: 40; test-2: 44\n",
    "Observation shape (12, 112, 128)\n",
    "Frame shape (112, 128, 3)\n",
    "Act value shape (1, 19)\n",
    "Compare observations: [33]:240.00\n",
    "Compare actions: [33]:12.00\n",
    "Compare values: [33]:1.88\n",
    "Compare frames: [34]:252.00\n",
    "\n",
    "\n",
    "the history given to the memory is exactly the same, difference occures in the observation (the input to the model), the difference in the frame only follow because the dfferent chosen actions.Also I remember in Atari games, the size of replay memory matters in evaluation, which makes no sense.\n",
    "\n",
    "Check replay memory, by:\n",
    "\n",
    "- Run the experiment once, and save the output of the replay memory for the model\n",
    "- In the second time of running the game, check the output for each time step\n",
    "\n",
    "We must save the effect in replay memory, otherwise, the \"done\" flags are random, and will seriously affect the encoded observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test saved trained models\n",
    "SUCCEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%run dqn_evaluate_contra.py\n",
    "stagefiles = [os.path.abspath('dqn_utils/nes_env/res/Contra_stage0_{}.nss'.format(i))\n",
    "              for i in range(4)]\n",
    "modelfiles = [os.path.abspath('checkpoints_contra_s0{}/cp{}.torchmodel'.format(i,ci))\n",
    "              for i, ci in zip(range(4), [2000000, 1750000, 2000000, 3500000])]\n",
    "for nssfile, mfile in zip(stagefiles, modelfiles):\n",
    "    RANDSEED=0\n",
    "    rng = np.random.RandomState(RANDSEED)\n",
    "    obs_rec, act_rec, actval_rec, frm_rec, rb1 = \\\n",
    "        test_repeat_model_eval(nssfile, mfile, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%run dqn_evaluate_contra.py\n",
    "stagefiles = [os.path.abspath('tmp/play1/s{}.nss'.format(i))\n",
    "              for i in range(4)]\n",
    "modelfiles = [os.path.abspath('checkpoints_contra_s0{}/cp{}.torchmodel'.format(i,ci))\n",
    "              for i, ci in zip(range(4), [2000000, 1750000, 2010000, 3500000])]\n",
    "for nssfile, mfile in zip(stagefiles, modelfiles):\n",
    "    RANDSEED=0\n",
    "    rng = np.random.RandomState(RANDSEED)\n",
    "    obs_rec, act_rec, actval_rec, frm_rec, rb1 = \\\n",
    "        test_repeat_model_eval(nssfile, mfile, rng)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
