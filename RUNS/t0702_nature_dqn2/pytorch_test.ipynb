{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run dqn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_HEIGHT = F_WIDTH = 84\n",
    "preproc = Preprocessor(F_HEIGHT, F_WIDTH)\n",
    "N = 3000\n",
    "env = Env(preproc, 4, 4)\n",
    "\n",
    "mem = ReplayMemory(N, F_HEIGHT, F_WIDTH, batch_size=32, frames_per_state=4)\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "state = env.reset()\n",
    "is_first_step = True\n",
    "positive_rewards_to_store = 2\n",
    "positive_rewards = 0\n",
    "for i in range(N):\n",
    "    action = rng.randint(2)\n",
    "    ab = 2 if action == 0 else 3\n",
    "    next_state, r, term = env.step(ab)\n",
    "    \n",
    "    round_done = (r != 0)  # for Pong\n",
    "    if not round_done and  term:\n",
    "        print \"Warning!\"\n",
    "        \n",
    "    if round_done and mem.term_[-1]:\n",
    "        print i, \"Repeat done!\"\n",
    "    mem.push(state, action, r, next_state, round_done, is_first_step)\n",
    "    \n",
    "    is_first_step = False\n",
    "\n",
    "    if term:\n",
    "        state = env.reset()\n",
    "    else:\n",
    "        state = next_state\n",
    "\n",
    "    if round_done or term:\n",
    "        is_first_step = True\n",
    "\n",
    "    # learning\n",
    "    if i % 500 == 0:\n",
    "        print '\\r{}'.format(i)\n",
    "        \n",
    "    if r>0:\n",
    "        positive_rewards += 1\n",
    "        print \"Got r>0 {} time(s)\".format(positive_rewards)\n",
    "        if positive_rewards >= positive_rewards_to_store:\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN((BATCH_SIZE, FRAMES_PER_STATE, F_HEIGHT, F_WIDTH))\n",
    "model_ = DQN((BATCH_SIZE, FRAMES_PER_STATE, F_HEIGHT, F_WIDTH))\n",
    "model.clone_to(model_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**2017-07-08**\n",
    "\n",
    "In the batch-processing script dqn.py, we had used a different set of parameters for RMSprop, could that be the problem? \n",
    "\n",
    "We didn't set eps and alpha in dqn.py, and set them when experimenting in the notebook.\n",
    "\n",
    "### RMSProp's parameters matter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss_fn = torch.nn.SmoothL1Loss()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optim_opts1 = {'lr':0.00025, 'momentum':MOMENTUM, 'eps':0.01, 'alpha':0.95}\n",
    "optim_opts2 = {'lr':0.00025, 'momentum':MOMENTUM}\n",
    "\n",
    "#optim = torch.optim.RMSprop(model.parameters(), **optim_opts2)\n",
    "#tmp_dir = 'tmp3'\n",
    "\n",
    "optim = torch.optim.RMSprop(model.parameters(), **optim_opts1)\n",
    "tmp_dir = 'tmp2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_train_round = 0\n",
    "while manual_train_round < 100:\n",
    "    ### a bit training\n",
    "    for i in range(100):\n",
    "        l = learn_step(model, model_, mem, optim, loss_fn)\n",
    "        if i%100 == 0: print \"\\r{}:{}\".format(i, l),\n",
    "    \n",
    "\n",
    "    manual_train_round += 1\n",
    "\n",
    "    ### evaluate\n",
    "    pred_state_act_values, act, reward, target_vals = evaluate(model, model_, mem)\n",
    "    pred_act_values = []\n",
    "    for v, a in zip(pred_state_act_values, act):\n",
    "        va = []\n",
    "        for v_, a_ in zip(v, a):\n",
    "            va.append([v_[0], v_[1], v_[a_],])\n",
    "        pred_act_values.append(np.asarray(va))\n",
    "\n",
    "    _ = model.clone_to(model_)\n",
    "    ### plot\n",
    "    episode_id = 0\n",
    "    epis = range(len(pred_act_values))\n",
    "    for episode_id in epis[-2:]:\n",
    "        clf()\n",
    "        plot(pred_act_values[episode_id][:,0], 'g:')\n",
    "        plot(pred_act_values[episode_id][:,1], 'g--')\n",
    "        plot(pred_act_values[episode_id][:,2], 'r')\n",
    "        plot(reward[episode_id], 'b')\n",
    "        plot(target_vals[episode_id], 'm--')\n",
    "        savefig('{}/{}_{}.png'.format(tmp_dir, manual_train_round, episode_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s0=rng.rand(4,8,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = rng.rand(8,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = np.concatenate((s0[1:], f[np.newaxis,...]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1[0]==s0[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i_, t1_, t2_ in zip(range(len(mem)-1), mem.term_[:-1], mem.term_[1:]):\n",
    "    if t1_ and t2_:\n",
    "        print i_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_act_values[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j in range(1000):\n",
    "    state_batch, action_batch, reward_batch, \\\n",
    "        next_state_batch, term_batch = \\\n",
    "        mem.make_batch(32)\n",
    "        \n",
    "    rr = reward_batch.cpu().numpy()\n",
    "    \n",
    "    if np.sum(rr>0)>0:\n",
    "        print np.nonzero(rr>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = DQN((BATCH_SIZE, FRAMES_PER_STATE, F_HEIGHT, F_WIDTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Variable((mem.obs_[10].float()/255.0).unsqueeze(0), volatile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.data[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reward_batch.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_batch, action_batch, reward_batch, next_state_batch, term_batch = mem.make_batch(32)\n",
    "batch_size=32\n",
    "target_reward = torch.zeros(batch_size).type(FloatTensor)\n",
    "pred_next_state_val = model(Variable(next_state_batch, volatile=True)).max(dim=1)[0] * 0.99\n",
    "target_reward[...] = reward_batch\n",
    "target_reward[1-term_batch] += pred_next_state_val.data[1-term_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lh = []\n",
    "model = DQN((BATCH_SIZE, FRAMES_PER_STATE, F_HEIGHT, F_WIDTH))\n",
    "model_ = DQN((BATCH_SIZE, FRAMES_PER_STATE, F_HEIGHT, F_WIDTH))\n",
    "model.clone_to(model_)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optim = torch.optim.RMSprop(model.parameters(), lr=1e-5, momentum=0.95)\n",
    "for i in range(300000):\n",
    "    ls = learn_step(model, model_, mem, optim, loss_fn)\n",
    "    lh.append(ls)\n",
    "    if i % 1000 == 0:\n",
    "        print \"\\r\",i, ls\n",
    "        model.clone_to(model_)\n",
    "L = np.asarray(lh).reshape(100,len(lh)/100)\n",
    "# plot(L.mean(axis=1))\n",
    "plot(lh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy.eps = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
