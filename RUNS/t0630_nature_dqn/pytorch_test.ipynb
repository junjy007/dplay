{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([1,2,3,4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "DocoptExit",
     "evalue": "Usage:\n    dqn.py learn | demo",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mDocoptExit\u001b[0m\u001b[0;31m:\u001b[0m Usage:\n    dqn.py learn | demo\n"
     ]
    }
   ],
   "source": [
    "%run dqn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-02 22:06:09,899] Making new env: Pong-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "Got r>0\n"
     ]
    }
   ],
   "source": [
    "F_HEIGHT = F_WIDTH = 84\n",
    "preproc = Preprocessor(F_HEIGHT, F_WIDTH)\n",
    "N = 3000\n",
    "env = Env(preproc, 4)\n",
    "\n",
    "mem = ReplayMemory(N, F_HEIGHT, F_WIDTH, batch_size=32, frames_per_state=4)\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "state = env.reset()\n",
    "is_first_step = True\n",
    "for i in range(N):\n",
    "    action = rng.randint(2)\n",
    "    ab = 2 if action == 0 else 3\n",
    "    next_state, r, term = env.step(ab)\n",
    "    \n",
    "    round_done = (r != 0)  # for Pong\n",
    "    if not round_done and  term:\n",
    "        print \"Warning!\"\n",
    "        \n",
    "    if round_done and mem.term_[-1]:\n",
    "        print i, \"Repeat done!\"\n",
    "    mem.push(state, action, r, next_state, round_done, is_first_step)\n",
    "    \n",
    "    is_first_step = False\n",
    "\n",
    "    if term:\n",
    "        state = env.reset()\n",
    "    else:\n",
    "        state = next_state\n",
    "\n",
    "    if round_done or term:\n",
    "        is_first_step = True\n",
    "\n",
    "    # learning\n",
    "    if i % 500 == 0:\n",
    "        print '.'\n",
    "        \n",
    "    if r>0:\n",
    "        print \"Got r>0\"\n",
    "        break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (32, 4, 84, 84)\n",
      "Feat torch.Size([32, 64, 7, 7])\n",
      "Input (32, 4, 84, 84)\n",
      "Feat torch.Size([32, 64, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "model = DQN((BATCH_SIZE, FRAMES_PER_STATE, F_HEIGHT, F_WIDTH))\n",
    "model_ = DQN((BATCH_SIZE, FRAMES_PER_STATE, F_HEIGHT, F_WIDTH))\n",
    "model.clone_to(model_)\n",
    "\n",
    "# loss_fn = torch.nn.SmoothL1Loss()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optim = torch.optim.RMSprop(model.parameters(), \n",
    "                            lr=0.025, momentum=MOMENTUM,\n",
    "                           eps=0.01, alpha=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900:0.026822961866964                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n"
     ]
    }
   ],
   "source": [
    "manual_train_round = 0\n",
    "while manual_train_round < 100:\n",
    "    ### a bit training\n",
    "    for i in range(1000):\n",
    "        l = learn_step(model, model_, mem, optim, loss_fn)\n",
    "        if i%100 == 0: print \"\\r{}:{}\".format(i, l),\n",
    "    \n",
    "\n",
    "    manual_train_round += 1\n",
    "\n",
    "    ### evaluate\n",
    "    pred_state_act_values, act, reward, target_vals = evaluate(model, model_, mem)\n",
    "    pred_act_values = []\n",
    "    for v, a in zip(pred_state_act_values, act):\n",
    "        va = []\n",
    "        for v_, a_ in zip(v, a):\n",
    "            va.append([v_[0], v_[1], v_[a_],])\n",
    "        pred_act_values.append(np.asarray(va))\n",
    "\n",
    "    _ = model.clone_to(model_)\n",
    "    ### plot\n",
    "    episode_id = 0\n",
    "    epis = range(len(pred_act_values))\n",
    "    for episode_id in epis[-2:]:\n",
    "        clf()\n",
    "        plot(pred_act_values[episode_id][:,0], 'g:')\n",
    "        plot(pred_act_values[episode_id][:,1], 'g--')\n",
    "        plot(pred_act_values[episode_id][:,2], 'r')\n",
    "        plot(reward[episode_id], 'b')\n",
    "        plot(target_vals[episode_id], 'm--')\n",
    "        savefig('tmp/{}_{}.png'.format(manual_train_round, episode_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0=rng.rand(4,8,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = rng.rand(8,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = np.concatenate((s0[1:], f[np.newaxis,...]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True]], dtype=bool)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1[0]==s0[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_, t1_, t2_ in zip(range(len(mem)-1), mem.term_[:-1], mem.term_[1:]):\n",
    "    if t1_ and t2_:\n",
    "        print i_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_act_values[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(1000):\n",
    "    state_batch, action_batch, reward_batch, \\\n",
    "        next_state_batch, term_batch = \\\n",
    "        mem.make_batch(32)\n",
    "        \n",
    "    rr = reward_batch.cpu().numpy()\n",
    "    \n",
    "    if np.sum(rr>0)>0:\n",
    "        print np.nonzero(rr>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN((BATCH_SIZE, FRAMES_PER_STATE, F_HEIGHT, F_WIDTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable((mem.obs_[10].float()/255.0).unsqueeze(0), volatile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.data[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_batch.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_batch, action_batch, reward_batch, next_state_batch, term_batch = mem.make_batch(32)\n",
    "batch_size=32\n",
    "target_reward = torch.zeros(batch_size).type(FloatTensor)\n",
    "pred_next_state_val = model(Variable(next_state_batch, volatile=True)).max(dim=1)[0] * 0.99\n",
    "target_reward[...] = reward_batch\n",
    "target_reward[1-term_batch] += pred_next_state_val.data[1-term_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lh = []\n",
    "model = DQN((BATCH_SIZE, FRAMES_PER_STATE, F_HEIGHT, F_WIDTH))\n",
    "model_ = DQN((BATCH_SIZE, FRAMES_PER_STATE, F_HEIGHT, F_WIDTH))\n",
    "model.clone_to(model_)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optim = torch.optim.RMSprop(model.parameters(), lr=1e-5, momentum=0.95)\n",
    "for i in range(300000):\n",
    "    ls = learn_step(model, model_, mem, optim, loss_fn)\n",
    "    lh.append(ls)\n",
    "    if i % 1000 == 0:\n",
    "        print \"\\r\",i, ls\n",
    "        model.clone_to(model_)\n",
    "L = np.asarray(lh).reshape(100,len(lh)/100)\n",
    "# plot(L.mean(axis=1))\n",
    "plot(lh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.eps = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
