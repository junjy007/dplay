{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "THIS_NOTEBOOK = 'nb01_reinforce_framework.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import imp  # Python 2\n",
    "from collections import deque\n",
    "import gym\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "BASEPATH = '/Users/junli/local/projects/dplay'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_tensor_f32(x):\n",
    "    t_ = torch.from_numpy(np.float32(np.ascontiguousarray(x)))\n",
    "    if USE_CUDA:\n",
    "        t_ = t_.cuda()\n",
    "    return t_\n",
    "\n",
    "def to_tensor_int(x):\n",
    "    t_ = torch.LongTensor(x)\n",
    "    if USE_CUDA:\n",
    "        t_ = t_.cuda()\n",
    "    return t_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_numpy(t):\n",
    "    if USE_CUDA:\n",
    "        x = t.cpu().numpy()\n",
    "    else:\n",
    "        x = t.numpy()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def running_val(running_v, v):\n",
    "    a = 0.99\n",
    "    return v if running_v is None \\\n",
    "        else running_v * a + v * (1.0 - a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites-END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data manage\n",
    "Work with Environment. Preprocess Data and Handle GPU CPU trans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FRAMEWORK_PREPROCESSOR: Stack-3-frames\n",
    "class Preprocessor:\n",
    "    \"\"\"\n",
    "    Raw pixel to numpy array as a \"single state observation\", which will be\n",
    "    dealt with by experience memory.\n",
    "    \n",
    "    This object will compare two frames, take the difference\n",
    "    \"\"\"\n",
    "    IM_WIDTH = 80\n",
    "    IM_HEIGHT = 80\n",
    "    def __init__(self):\n",
    "        self.previous_frame_2 = None\n",
    "        self.previous_frame_1 = None\n",
    "        \n",
    "    def __call__(self, I):\n",
    "        return self.process(I)\n",
    "\n",
    "    # noinspection PyPep8Naming,PyMethodMayBeStatic\n",
    "    def process(self, I):\n",
    "        \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "        I = I[35:195]  # crop\n",
    "        I = I[::2, ::2, 0]  # downsample by factor of 2\n",
    "        I[I == 144] = 0  # erase background (background type 1)\n",
    "        I[I == 109] = 0  # erase background (background type 2)\n",
    "        I[I != 0] = 1  # everything else (paddles, ball) just set to 1\n",
    "        I = np.float32(I)\n",
    "        if self.previous_frame_1 is None:\n",
    "            s = np.stack((I, I, I))\n",
    "            self.previous_frame_1 = I\n",
    "            self.previous_frame_2 = I\n",
    "        else:\n",
    "            s = np.stack((self.previous_frame_2, self.previous_frame_1, I))\n",
    "            self.previous_frame_2 = self.previous_frame_1\n",
    "            self.previous_frame_1 = I\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FRAMEWORK_ENVIRONMENT: Pong-v0\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self._env = gym.make('Pong-v0')\n",
    "        self._stopped = False\n",
    "\n",
    "    def reset(self):\n",
    "        self._stopped = False\n",
    "        return self._env.reset()\n",
    "    \n",
    "    def step(self, act):\n",
    "        if not self._stopped:\n",
    "            s, r, t, info = self._env.step(act)\n",
    "            if r!=0:\n",
    "                t = True\n",
    "            if t:\n",
    "                self._stopped = True\n",
    "        else:\n",
    "            s = r = info = None\n",
    "        return s, r, self._stopped, info\n",
    "    \n",
    "    def render(self):\n",
    "        return self._env.render()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ExperienceMemory:\n",
    "    \"\"\"\n",
    "    At a time step t, this memory manager expects (args to add_experience):\n",
    "        S_{t-1}, A_{t-1}, R_t, Is_Terminal_{t}, Agent_Response_To(S_{t-1})\n",
    "    i.e. the arguments (numbers) are\n",
    "                t-1  | t\n",
    "    - STATE:      1  |\n",
    "    - ACTION:     2  |\n",
    "    - REWARD:        | 3\n",
    "    - TERMINAL:      | 4\n",
    "    - RESP:         =5=>\n",
    "    and\n",
    "    - ADVANTAGE_{t}: = Reward_t + Reward_{t+1}*discount + ...\n",
    "      will be filled when episode ends\n",
    "\n",
    "    NB   S_t (the current state) will be added in the next step. The last state of an episode is\n",
    "      NOT recorded (no action will be taken, after all!)\n",
    "    NB-1 All info is not used in all learning algorithms\n",
    "    NB-2 RESP will be saved to time step t. This is to align with the supervision information\n",
    "         that will finally arrive to train the agent. E.g. in Q-learning, the agent would\n",
    "         try to evaluate all actions given state S_{t-1}, the evaluation will be comapred\n",
    "         to reward received at time {t}.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity, discount):\n",
    "        self.capacity = capacity\n",
    "        self.discount = discount\n",
    "\n",
    "        self.experience = {\n",
    "            'states': deque(),\n",
    "            'actions': deque(),\n",
    "            'rewards': deque(),\n",
    "            'advantages': deque(),\n",
    "            'term_status': deque(),\n",
    "            'prev_responses': deque(),\n",
    "            'episode_id': deque()\n",
    "        }\n",
    "        self.first_step_in_episode = True\n",
    "\n",
    "    def add_experience(self, episode_id, state, action, reward, is_terminal,\n",
    "                       prev_resp=None, do_compute_advantage=True):\n",
    "        \"\"\"\n",
    "        :param episode_id: do NOT record the episode ID in this object, which can cause inconsistence\n",
    "          when save/load training sessions.\n",
    "        :param state: See class doc, the last step state.\n",
    "        :type state: np.ndarray\n",
    "        :param action:\n",
    "        :type action: int\n",
    "        :param reward:\n",
    "        :param is_terminal:\n",
    "        :param prev_resp:\n",
    "        :param do_compute_advantage: if set, I will compute discounted advantage (see\n",
    "          class doc) when an episode ends.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # if self.first_step_in_episode:\n",
    "        #     self.first_step_in_episode = False\n",
    "        #     self.experience['states'].append(state)\n",
    "        #     assert new_state is None\n",
    "        #     assert action is None\n",
    "        #     assert reward == 0\n",
    "        #     assert not is_terminal\n",
    "        #     assert prev_resp is None\n",
    "        # else:\n",
    "        #    # eventually, \"actions\" has one less element than other records\n",
    "\n",
    "        self.experience['states'].append(state)\n",
    "        self.experience['actions'].append(action)\n",
    "        self.experience['rewards'].append(reward)\n",
    "        self.experience['term_status'].append(is_terminal)\n",
    "        self.experience['advantages'].append(0)\n",
    "        self.experience['prev_responses'].append(prev_resp)\n",
    "        self.experience['episode_id'].append(episode_id)\n",
    "\n",
    "        N = len(self.experience['states'])\n",
    "        if N > self.capacity:\n",
    "            for k_ in self.experience:\n",
    "                self.experience[k_].popleft()\n",
    "            N -= 1\n",
    "\n",
    "        if is_terminal and do_compute_advantage:\n",
    "            self.experience['advantages'][-1] = future_reward = reward\n",
    "            this_episode = self.experience['episode_id'][-1]\n",
    "            for t in range(-2, - N - 1, -1):\n",
    "                if self.experience['episode_id'][t] != this_episode:\n",
    "                    break  # have reached the previous episode\n",
    "                self.experience['advantages'][t] = \\\n",
    "                    self.experience['rewards'][t] + future_reward * self.discount\n",
    "                future_reward = self.experience['advantages'][t]\n",
    "\n",
    "    def get_training_batch(self, num_steps=None, episodes=None):\n",
    "        \"\"\"\n",
    "        Specific to the RL algorithm. This implementation is for Policy Gradient using a single episode\n",
    "        \"\"\"\n",
    "        s_ = not (num_steps is None)\n",
    "        e_ = not (episodes is None)\n",
    "        assert (s_ and not e_) or (not s_ and e_), \\\n",
    "            \"Either the number of steps or the episode id list must be given\"\n",
    "\n",
    "        # implement only using episode\n",
    "        states = []\n",
    "        actions = []\n",
    "        advantages = []\n",
    "\n",
    "        if s_:\n",
    "            assert False, \"Not implemented\"\n",
    "\n",
    "        if e_:\n",
    "            for ep in episodes:\n",
    "                ids = [i_ for i_, ei_ in enumerate(self.experience['episode_id'])\n",
    "                       if ei_ == ep]\n",
    "\n",
    "                for t in ids:  # no last action\n",
    "                    states.append(self.experience['states'][t])\n",
    "                    actions.append(self.experience['actions'][t])\n",
    "                    advantages.append(self.experience['advantages'][t])\n",
    "\n",
    "        return to_tensor_f32(states), to_tensor_int(actions), to_tensor_f32(advantages)\n",
    "    \n",
    "    def get_next_training_batch(self):\n",
    "        \"\"\"\n",
    "        A short cut for taking the last one episode\n",
    "        \"\"\"\n",
    "        return self.get_training_batch(\n",
    "            episodes=[self.experience['episode_id'][-1], ])\n",
    "\n",
    "    def save(self, fname):\n",
    "        with open(fname, 'w') as f:\n",
    "            f.write('Nothing to save')\n",
    "\n",
    "    def load(self, fname):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test Memory and Environment - 1, Save experience\n",
    "%pylab inline\n",
    "preproc = Preprocessor()\n",
    "env = Environment()\n",
    "mem = ExperienceMemory(500, 0.99)\n",
    "s = preproc(env.reset())\n",
    "ei = 0\n",
    "t = 0\n",
    "while ei<10:\n",
    "    print \"\\rt {}\".format(t),\n",
    "    sys.stdout.flush()\n",
    "    s1, r, term, _ = env.step(1)\n",
    "    s1 = preproc(s1)\n",
    "    mem.add_experience(ei, s, 1, r, term)\n",
    "    \n",
    "    if term:\n",
    "        s = preproc(env.reset())\n",
    "        ei += 1\n",
    "    else:\n",
    "        s = s1\n",
    "    env.render()\n",
    "    t += 1\n",
    "    \n",
    "plot(mem.experience['advantages'])\n",
    "plot(mem.experience['rewards'], 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test Memory and Environment - 2: Sanity check.\n",
    "test_states, test_actions, test_advs = mem.get_training_batch(episodes=[8,9])\n",
    "print test_states.numpy().shape\n",
    "plot(test_advs.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks\n",
    "The neural network that takes states and produces desired assessments. A network consists of two parts: encoder and decoder:\n",
    "- Encoder: This part of the model is generic -- once the extractor has been learned, it can be adapted to other tasks with difference format of inputs (same number of channels) and it is independent with the task-specific target. See below.\n",
    "- Decoder: It takes the features and procudes the outputs, e.g. in Q-learning the targets are action values, in policy gradient, the targets are next action probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FRAMEWORK_ENCODER: Simple-conv-v0\n",
    "\n",
    "# opts['input_channels'] = FRAMES_PER_STATE * INPUT_CHANNELS\n",
    "class DeepConvEncoder(nn.Module):\n",
    "    def __init__(self, opts):\n",
    "        super(DeepConvEncoder, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.conv_layers = []\n",
    "        self.input_channels = opts['input_channels']\n",
    "        in_kernels = self.input_channels\n",
    "        \n",
    "        for cf in opts['convs']:\n",
    "            ks = cf['kernel_size']\n",
    "            kn = cf['conv_kernels']\n",
    "            \n",
    "            lay_ = []\n",
    "            lay_.append(nn.Conv2d(in_channels=in_kernels, \n",
    "                      out_channels=kn, \n",
    "                      kernel_size=ks, padding=(ks - 1) / 2))\n",
    "            if cf['relu']:\n",
    "                lay_.append(nn.ReLU())\n",
    "            lay_.append(nn.MaxPool2d(kernel_size=cf['pool_factor']))\n",
    "            self.conv_layers.append( nn.Sequential(*lay_) )\n",
    "            in_kernels = kn\n",
    "            \n",
    "        self.feature = nn.Sequential(*self.conv_layers)\n",
    "        self.num_features = None\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            self.cuda()\n",
    "        \n",
    "    def get_feature_num(self, image_size=None):\n",
    "        \"\"\"\n",
    "        :param image_size: info about state variable of images, see Preprocessor and ExperienceMemory\n",
    "            ['height/width']\n",
    "        \"\"\"\n",
    "        # TODO: Maybe Cuda dummy variable is needed.\n",
    "        if self.num_features is None:\n",
    "            assert not (image_size is None), \"Image size must be given in the first time\"\n",
    "            dummy_input = Variable(torch.rand(1, self.input_channels, \n",
    "                                              image_size['height'], image_size['width']))\n",
    "            dummy_feature = self.feature(dummy_input)\n",
    "            nfeat = np.prod(dummy_feature.size()[1:])\n",
    "            self.num_features = nfeat\n",
    "        return self.num_features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.feature(x)\n",
    "    \n",
    "    def save(self, fname):\n",
    "        torch.save(self.state_dict(), fname)\n",
    "        \n",
    "    def load(self, fname):\n",
    "        self.load_state_dict(torch.load(fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FRAMEWORK_DECODER: Policy-4-actions\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, opts):  # TODO use decoder opts\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_num = opts['input_num']\n",
    "        \n",
    "        self._fc1 = nn.Linear(in_features=opts['input_num'], \n",
    "                              out_features=opts['fc1_hidden_unit_num'])\n",
    "        self._fc2 = nn.Linear(in_features=opts['fc1_hidden_unit_num'], \n",
    "                              out_features=opts['output_num'])\n",
    "        self._fullconn = nn.Sequential(self._fc1, self._fc2, nn.LogSoftmax())\n",
    "        # LogSoftmax -- to comply with NLLLoss, which expects the LOG of predicted\n",
    "        # probability and the target\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self._fullconn(x)\n",
    "    \n",
    "    def save(self, fname):\n",
    "        torch.save(self.state_dict(), fname)\n",
    "        \n",
    "    def load(self, fname):\n",
    "        self.load_state_dict(torch.load(fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FRAMEWORK_RLNET: RLNet-v0\n",
    "class RLNet(nn.Module):\n",
    "    def __init__(self, enc, dec):\n",
    "        \"\"\"\n",
    "        :param enc: Feature extractor. See Encoder.\n",
    "        :type enc: Encoder\n",
    "        :param dec: Task target predictor\n",
    "        :type dec: Decoder\n",
    "        \"\"\"\n",
    "        super(RLNet, self).__init__()\n",
    "        assert enc.get_feature_num() == dec.input_num\n",
    "        self.enc = enc\n",
    "        self.dec = dec\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.enc(x)\n",
    "        y = y.view(-1, self.enc.get_feature_num())\n",
    "        y = self.dec(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "encoder_opts = {\n",
    "    'input_channels': 3,\n",
    "    'convs': [\n",
    "        {'kernel_size':3, 'conv_kernels': 32, 'pool_factor': 2, 'relu': True},\n",
    "        {'kernel_size':3, 'conv_kernels': 64, 'pool_factor': 2, 'relu': True},\n",
    "    ]\n",
    "}\n",
    "\n",
    "encoder = DeepConvEncoder(encoder_opts)\n",
    "\n",
    "nin = encoder.get_feature_num({'height':preproc.IM_HEIGHT, 'width':preproc.IM_WIDTH})\n",
    "\n",
    "decoder_opts = {\n",
    "    'input_num': nin,\n",
    "    'fc1_hidden_unit_num': 256,\n",
    "    'output_num':4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(decoder_opts)\n",
    "net = RLNet(encoder, decoder)\n",
    "y = net(Variable(test_states, requires_grad=False))\n",
    "yv = y.data.numpy()\n",
    "print yv.shape\n",
    "print yv[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy\n",
    "Policies select an action for a state. The state is given in a preprocessed form that is ready to be taken by an RLNet object, which produces assessment of the state. Policy then chooses an action accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FRAMEWORK_POLICY: PG-v0\n",
    "class Policy:\n",
    "    \"\"\"\n",
    "    This policy is to be used with Policy Gradient. The RLNet outputs action probabilities, which \n",
    "    stochastically determines the action.\n",
    "    \"\"\"\n",
    "    def __init__(self, rl_net):\n",
    "        self.rl_net = rl_net\n",
    "        self.rng = np.random.RandomState(0)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        :param state: a single frame\n",
    "        :type state: np.ndarray\n",
    "        \"\"\"\n",
    "        assert state.ndim == 3, \\\n",
    "            \"Single state required, multi-state not implemented\"\n",
    "        state_s = state[np.newaxis, ...]\n",
    "        state_tv_ = to_tensor_f32(state_s)\n",
    "        action_prob = self.rl_net(Variable(state_tv_, requires_grad=False))\n",
    "        action_prob = np.exp(to_numpy(action_prob.data)).squeeze()\n",
    "        return self.rng.choice(action_prob.size, p=action_prob), action_prob\n",
    "        \n",
    "        \n",
    "    def save(self, fname):\n",
    "        with open(fname, 'w') as f:\n",
    "            f.write('Nothing to save')\n",
    "    \n",
    "    def load(self, fname):\n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sanity check of policy\n",
    "po = Policy(net)\n",
    "s_ = preproc(env.reset())\n",
    "print po.get_action(s_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "Trainer takes recent experience, adjust model parameters to minimise a loss. Hopefully, a smaller loss will lead to a better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OneStepPolicyGradientTrainer:\n",
    "    def __init__(self, net, memory, opts):\n",
    "        \"\"\"\n",
    "        :type net:\n",
    "        \"\"\"\n",
    "        self.optimiser = opts['Optimiser'](\n",
    "            net.parameters(),   # e.g. torch.optim.Adam()\n",
    "            lr=opts['learning_rate'])\n",
    "        self.loss_fn = nn.NLLLoss(size_average=False)\n",
    "        self.net = net\n",
    "        self.memory = memory\n",
    "        \n",
    "    def __call__(self):\n",
    "        return self.step()\n",
    "        \n",
    "    def step(self):\n",
    "        states, actions, advantages = (\n",
    "            Variable(t_, requires_grad=False) \n",
    "            for t_ in self.memory.get_next_training_batch()\n",
    "        )\n",
    "        logP = self.net(states)                       # predicted log-prob of taking different actions\n",
    "        advantages.data -= advantages.data.mean()     # An operation for tensors not Variables\n",
    "        advantages = advantages.unsqueeze(1)          # -> sample_num x 1\n",
    "        advantages = advantages.expand(\n",
    "            advantages.size(0), \n",
    "            logP.size(1))                   # \"manual\" broadcasting, -> #.samples x #.classes\n",
    "        logP_adv = advantages * logP\n",
    "        \n",
    "        loss = self.loss_fn(logP_adv, actions)        # Integers for 2nd arg, 'target'\n",
    "        \n",
    "        # Back-prop\n",
    "        self.optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimiser.step()\n",
    "        \n",
    "        loss_value = to_numpy(loss.data)[0]  # we will return a float, not a single-element array\n",
    "        return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sanity check of trainer.\n",
    "opts_ = {'Optimiser': torch.optim.Adagrad, 'learning_rate':1e-6}\n",
    "trainer = OneStepPolicyGradientTrainer(net, mem, opts_)\n",
    "trainer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLLoss is defined as \n",
    "$$\n",
    "\\sum_n - \\log P_{n_i}\n",
    "$$\n",
    "where $n_i$ is the actual class for the $n$-th sample and $P$ is the predicted prob. To minimise the negative value of the log-probability is to push the network so the probability of the classes tha actually happen increases. I.e. when $n_i$-th class is the case for $n$-th sample, you'd like the model to predict more chance of class $n_i$ for the $n$-th sample next time. \n",
    "\n",
    "In RL, we introduce the concept of {\\em advantage}: instead of increasing the likelihood of acutal action, we allow the probability to go both ways -- it get increased if the chosen action turns out to be a good one, and on the contrary, for decisions turns to be bad, it can decrease its future probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeper\n",
    "A Keeper maintains information about the training, such as how many epoches, episodes, minibatches. The methods can be thought as **callbacks** -- to be invoked by the learning algorithm at various occasions, such as when and how to save/load models, when to stop and when to perform evaluation etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Keeper:\n",
    "    \"\"\"\n",
    "    Keeper helps administrate learning:\n",
    "    ** Methods\n",
    "    - save              and\n",
    "    - load              handle checkpoints in the training sessions\n",
    "    - record_env_step   and\n",
    "    - record_train_step keeps records of training progress\n",
    "    - report_step       prints recent progress\n",
    "    ** Flags\n",
    "    - need_train        means: according to the learning strategy,\n",
    "      enough experience has been accquired, a training step is ready\n",
    "      to run\n",
    "    - need_save         enough training steps done, to save checkpoint\n",
    "    - need_draw         to show the game in one episode\n",
    "    \"\"\"\n",
    "    def __init__(self, objs, opts):\n",
    "        \"\"\"\n",
    "        :param objs: objects that can be saved and reloaded in training sessions.\n",
    "        :param opts: e.g. {\n",
    "            'train_every_n_episodes': 1,\n",
    "            'save_every_n_training_steps': 10,  # TODO: set to resonably large\n",
    "            'draw_every_n_training_steps': -1,\n",
    "            'save_path': 'SAVE', \n",
    "            'report': {\n",
    "                'save_checkpoint': True,\n",
    "                'every_n_steps': 1,\n",
    "                'every_n_training': 1,\n",
    "                'every_n_episodes': 1}\n",
    "            }\n",
    "        \n",
    "        \"\"\"\n",
    "        self.opts = opts\n",
    "        self.objects = objs\n",
    "        self.object_filenames = [o_.__class__.__name__ for o_ in self.objects]\n",
    "        for i in range(len(self.object_filenames)):\n",
    "            while self.object_filenames[i] in self.object_filenames[:i]:\n",
    "                self.object_filenames[i] += '-copy'\n",
    "\n",
    "        self.savepath = opts['save_path']\n",
    "        self.checkpoint_status_fullfname = os.path.join(self.savepath, 'latest.json')\n",
    "        self.records = {\n",
    "            'total_steps': 0,  # step-wise info\n",
    "            'reward_history': [],\n",
    "            'running_reward': None,\n",
    "            'episodes': 0,  # episode-wise info\n",
    "            'episode_length_history': [],\n",
    "            'episode_reward_history': [],\n",
    "            'running_episode_reward': None,\n",
    "            'training_steps': 0,  # mini-batch-wise info\n",
    "            'loss_history': [],  # training stuff goes here\n",
    "            'running_loss': None,\n",
    "            'checkpoint_history': [],  # checkpoints\n",
    "            'latest_checkpoint': ''\n",
    "        }\n",
    "        self.last_term_t = 0\n",
    "        self.need_train = False\n",
    "        self.need_save = False\n",
    "        self.need_draw = False  # NOTE this flag is reset by renderer\n",
    "        self.need_stop = False\n",
    "\n",
    "        self.report_opts = self.opts['report']\n",
    "        self.last_reported_episode = -1\n",
    "        self.last_reported_training = -1\n",
    "\n",
    "    def save(self):\n",
    "        checkpoint_path = os.path.join(self.savepath,\n",
    "                                       'checkpoint-{:d}'.format(self.records['training_steps']))\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            os.mkdir(checkpoint_path)\n",
    "        self.records['checkpoint_history'].append(checkpoint_path)\n",
    "        \n",
    "        obj_full_fnames = [os.path.join(checkpoint_path, f_) \n",
    "                           for f_ in self.object_filenames]\n",
    "        for o, of in zip(self.objects, obj_full_fnames):\n",
    "            o.save(of)\n",
    "            # print \"Save an object {} to {}\".format(o.__class__.__name__, of)\n",
    "        \n",
    "        self.records['latest_checkpoint'] = checkpoint_path\n",
    "        with open(self.checkpoint_status_fullfname, 'w') as f:\n",
    "            json.dump(self.records, f, indent=2)\n",
    "\n",
    "        self.need_save = False\n",
    "        if self.report_opts['save_checkpoint']:\n",
    "            print 'Checkpoint {} saved to {}'.format(\n",
    "                len(self.records['checkpoint_history']),\n",
    "                self.records['checkpoint_history']\n",
    "            )\n",
    "\n",
    "    def load(self):\n",
    "        if not os.path.exists(self.checkpoint_status_fullfname):\n",
    "            return\n",
    "\n",
    "        with open(self.checkpoint_status_fullfname, 'r') as f:\n",
    "            self.records = json.load(f)\n",
    "\n",
    "        obj_full_fnames = [os.path.join(self.records['latest_checkpoint'], f_) \n",
    "                           for f_ in self.object_filenames]\n",
    "\n",
    "        for o, of in zip(self.objects, obj_full_fnames):\n",
    "            o.load(of)\n",
    "            # print \"Load an object {} from {}\".format(o.__class__.__name__, of)\n",
    "\n",
    "    def record_env_step(self, reward, term):\n",
    "        r = float(reward)\n",
    "        rec = self.records\n",
    "        rec['reward_history'].append(r)\n",
    "        rec['running_reward'] = running_val(rec['running_reward'], r)\n",
    "        rec['total_steps'] += 1\n",
    "        if term:\n",
    "            # --------\n",
    "            # NOT save full reward history in JSON. Clear for now\n",
    "            # ep_r = np.sum(rec['reward_history'][self.last_term_t:])\n",
    "            ep_r = np.sum(rec['reward_history'])\n",
    "            rec['reward_history'] = []\n",
    "            # --------\n",
    "            rec['episode_reward_history'].append(ep_r)\n",
    "            rec['running_episode_reward'] = running_val(rec['running_episode_reward'], ep_r)\n",
    "            rec['episode_length_history'].append(rec['total_steps'] - self.last_term_t)\n",
    "            self.last_term_t = rec['total_steps']\n",
    "            rec['episodes'] += 1\n",
    "            if rec['episodes'] % self.opts['train_every_n_episodes'] == 0:\n",
    "                self.need_train = True  # reset when recording a training step\n",
    "            self.need_draw = False\n",
    "\n",
    "    def record_train_step(self, loss):\n",
    "        \"\"\"\n",
    "        Record training loss. This is separate from recording environment rewards because\n",
    "        training step happens every few steps.\n",
    "        \"\"\"\n",
    "        loss = float(loss)\n",
    "        self.need_train = False\n",
    "        rec = self.records\n",
    "        rec['loss_history'].append(loss)\n",
    "        rec['running_loss'] = running_val(rec['running_loss'], loss)\n",
    "        rec['training_steps'] += 1\n",
    "        k_ = self.opts['save_every_n_training_steps']\n",
    "        if k_ > 0 and rec['training_steps'] % k_ == 0:\n",
    "            self.need_save = True\n",
    "        k_ = self.opts['draw_every_n_training_steps']\n",
    "        if k_ > 0 and rec['training_steps'] % k_ == 0:\n",
    "            self.need_draw = True  # reset when an episode ends\n",
    "        if rec['training_steps'] >= self.opts['max_training_steps']:\n",
    "            self.need_stop = True  # TODO: use more elegant conditions\n",
    "\n",
    "    def report_step(self):\n",
    "        did_rep = False\n",
    "        # agent-environment interaction steps -- fastest changing\n",
    "        n = self.report_opts['every_n_steps']\n",
    "        N = self.records['total_steps']\n",
    "        if n > 0 and N > 0 and N % n == 0:\n",
    "            print \"Total step {}, reward {:.3f}, running_reward {:.3f} \".format(\n",
    "                N,\n",
    "                self.records['reward_history'][-1],\n",
    "                self.records['running_reward']),\n",
    "            did_rep = True\n",
    "\n",
    "        # every a few episodes -- the reward for episodes is what we are really concerned\n",
    "        n = self.report_opts['every_n_episodes']\n",
    "        N = self.records['episodes']\n",
    "        if n > 0 and N > 0 and N % n == 0 and N != self.last_reported_episode:\n",
    "            print \"Episode {} steps {} reward {:.3f} running episode reward {:.3f} \".format(\n",
    "                N,\n",
    "                self.records['episode_length_history'][-1],\n",
    "                self.records['episode_reward_history'][-1],\n",
    "                self.records['running_episode_reward']),\n",
    "            self.last_reported_episode = N\n",
    "            did_rep = True\n",
    "\n",
    "        # training\n",
    "        n = self.report_opts['every_n_training']\n",
    "        N = self.records['training_steps']\n",
    "        if n > 0 and N > 0 and N % n == 0 and N != self.last_reported_training:\n",
    "            print \"Training step {}, loss {:.3f}, running_loss {:.3f} \".format(\n",
    "                N,\n",
    "                self.records['loss_history'][-1],\n",
    "                self.records['running_loss']\n",
    "            ),\n",
    "            self.last_reported_training = N\n",
    "            did_rep = True\n",
    "\n",
    "        if did_rep:\n",
    "            print\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sanity check of Keeper\n",
    "# 1. setup\n",
    "preproc = Preprocessor()\n",
    "env = Environment()\n",
    "mem = ExperienceMemory(500, 0.99)\n",
    "encoder_opts = {\n",
    "    'input_channels': 3,\n",
    "    'convs': [\n",
    "        {'kernel_size':3, 'conv_kernels': 32, 'pool_factor': 2, 'relu': True},\n",
    "        {'kernel_size':3, 'conv_kernels': 64, 'pool_factor': 2, 'relu': True},\n",
    "    ]\n",
    "}\n",
    "\n",
    "encoder = DeepConvEncoder(encoder_opts)\n",
    "\n",
    "nin = encoder.get_feature_num({'height':preproc.IM_HEIGHT, 'width':preproc.IM_WIDTH})\n",
    "\n",
    "decoder_opts = {\n",
    "    'input_num': nin,\n",
    "    'fc1_hidden_unit_num': 256,\n",
    "    'output_num':4\n",
    "}\n",
    "\n",
    "decoder = Decoder(decoder_opts)\n",
    "net = RLNet(encoder, decoder)\n",
    "\n",
    "po = Policy(net)\n",
    "\n",
    "trainer = OneStepPolicyGradientTrainer(net, mem, \n",
    "    {'Optimiser': torch.optim.Adagrad, 'learning_rate':1e-6})\n",
    "\n",
    "path_opts = {\n",
    "    'BASE_PATH': '/Users/junli/local/projects/dplay',\n",
    "    'RUN_PATH': 'RUNS',\n",
    "    'experiment_id': 'TEST01_sanitychk'}\n",
    "\n",
    "running_dir = os.path.join(path_opts['BASE_PATH'], \n",
    "                           path_opts['RUN_PATH'], \n",
    "                           path_opts['experiment_id'])\n",
    "\n",
    "save_dir = os.path.join(running_dir, 'checkpoints')\n",
    "\n",
    "if not os.path.exists(running_dir):\n",
    "    os.mkdir(running_dir)  # NOT using makedirs, I want the \n",
    "    # users to be responsible for the parent directory (and \n",
    "    # overall structure)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "keeper = Keeper([encoder, decoder, po, mem], {\n",
    "    'train_every_n_episodes': 1,\n",
    "    'save_every_n_training_steps': 10,\n",
    "    'draw_every_n_training_steps': -1,\n",
    "    'max_training_steps': 100,\n",
    "    'save_path': save_dir,\n",
    "    'report': {\n",
    "                'save_checkpoint': True,\n",
    "                'every_n_steps': 1,\n",
    "                'every_n_training': 1,\n",
    "                'every_n_episodes': 1}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2. get some data for training\n",
    "state = preproc.process(env.reset())\n",
    "\n",
    "while not keeper.need_train:\n",
    "    action, action_prob = po.get_action(state)\n",
    "    next_state, reward, is_terminal, _ = env.step(action)\n",
    "    next_state = preproc.process(next_state)\n",
    "    ep = keeper.records['episodes']\n",
    "    mem.add_experience(ep, state, action, reward, is_terminal, None)\n",
    "    keeper.record_env_step(reward, is_terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3. do training\n",
    "for i in range(10):\n",
    "    loss = trainer.step()\n",
    "    keeper.record_train_step(loss)\n",
    "    print i, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4. save and load\n",
    "keeper.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GO ABOVE, re-initialise the encoder/decoder / re-collect experience, see if\n",
    "# the training starts from where it is supposed to \n",
    "keeper.load()\n",
    "for i in range(10):\n",
    "    loss = trainer.step()\n",
    "    keeper.record_train_step(loss)\n",
    "    print i, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keeper.load()\n",
    "state = preproc.process(env.reset())\n",
    "while not keeper.need_stop:\n",
    "    action, action_prob = po.get_action(state)\n",
    "    next_state, reward, is_terminal, _ = env.step(action)\n",
    "    next_state = preproc.process(next_state)\n",
    "    mem.add_experience(state, action, reward, is_terminal, None)\n",
    "    if is_term:\n",
    "        state = preproc.process(env.reset())\n",
    "    else:\n",
    "        state = next_state\n",
    "    \n",
    "    keeper.record_env_step(reward, is_terminal)\n",
    "    \n",
    "    if keeper.need_train:  # TODO train condition call back\n",
    "        loss = train_step()\n",
    "        keeper.record_train_step(loss)\n",
    "        \n",
    "    if keeper.need_save:\n",
    "        keeper.save()\n",
    "        \n",
    "    if keeper.need_draw:\n",
    "        env.render()\n",
    "        \n",
    "    keeper.report_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components-END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framework-F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FRAMEWORK_RL\n",
    "RL_components = {\n",
    "    'Preprocessor': Preprocessor,\n",
    "    'ExperienceMemoryManager': ExperienceMemory,\n",
    "    'Encoder': DeepConvEncoder,\n",
    "    'Decoder': Decoder,\n",
    "    'RLNet': RLNet,\n",
    "    'Policy': Policy,\n",
    "    'Environment': Environment,\n",
    "    'Trainer': OneStepPolicyGradientTrainer,\n",
    "    'Keeper': Keeper,\n",
    "}\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "experience_opts = {\n",
    "    'capacity': 1000,\n",
    "    'discount': 0.99\n",
    "}\n",
    "\n",
    "encoder_opts = {\n",
    "    'input_channels': 3,\n",
    "    'convs': [\n",
    "        {'kernel_size':3, 'conv_kernels': 32, 'pool_factor': 2, 'relu': True},\n",
    "        {'kernel_size':3, 'conv_kernels': 64, 'pool_factor': 2, 'relu': True},\n",
    "    ]\n",
    "}\n",
    "\n",
    "decoder_opts = {\n",
    "    'input_num': None,\n",
    "    'fc1_hidden_unit_num': 256,\n",
    "    'output_num':4\n",
    "}\n",
    "\n",
    "trainer_opts = {'Optimiser': torch.optim.Adagrad, 'learning_rate':1e-6}\n",
    "\n",
    "path_opts = {\n",
    "    'BASE_PATH': BASEPATH,\n",
    "    'RUN_PATH': 'RUNS',\n",
    "    'experiment_id': 'TEST01'}\n",
    "\n",
    "running_dir = os.path.join(path_opts['BASE_PATH'], \n",
    "                           path_opts['RUN_PATH'], \n",
    "                           path_opts['experiment_id'])\n",
    "\n",
    "save_dir = os.path.join(running_dir, 'checkpoints')\n",
    "\n",
    "if not os.path.exists(running_dir):\n",
    "    os.mkdir(running_dir)  # NOT using makedirs, I want the \n",
    "    # users to be responsible for the parent directory (and \n",
    "    # overall structure)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "keeper_opts = {\n",
    "    'train_every_n_episodes': 1,\n",
    "    'save_every_n_training_steps': 10,\n",
    "    'draw_every_n_training_steps': -1,\n",
    "    'max_training_steps': 100,\n",
    "    'save_path': save_dir,\n",
    "    'report': {'save_checkpoint': True,\n",
    "               'every_n_steps': -1,\n",
    "               'every_n_training': 1,\n",
    "               'every_n_episodes': 1}\n",
    "}\n",
    "    \n",
    "# CREATE LEARNING COMPONENTS\n",
    "env = RL_components['Environment']()\n",
    "preproc = RL_components['Preprocessor']()\n",
    "mem = RL_components['ExperienceMemoryManager'](**experience_opts)\n",
    "enc = RL_components['Encoder'](encoder_opts)\n",
    "decoder_opts['input_num'] = enc.get_feature_num({'height':preproc.IM_HEIGHT, 'width':preproc.IM_WIDTH})\n",
    "dec = RL_components['Decoder'](decoder_opts)\n",
    "rlnet = RL_components['RLNet'](enc, dec)\n",
    "policy = RL_components['Policy'](rlnet)\n",
    "trainer = RL_components['Trainer'](rlnet, mem, trainer_opts)\n",
    "keeper = RL_components['Keeper']([enc, dec, policy, mem], keeper_opts)  # objects has \"save/load\" interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keeper.load()\n",
    "state = preproc.process(env.reset())\n",
    "\n",
    "while not keeper.need_stop:  \n",
    "    action, action_prob = policy.get_action(state)\n",
    "    next_state, reward, is_terminal, _ = env.step(action)\n",
    "    next_state = preproc(next_state)\n",
    "    ep = keeper.records['episodes']\n",
    "    mem.add_experience(ep, state, action, reward, is_terminal, None)\n",
    "    # None: We don't use last prediction (will predict in traing step)\n",
    "    \n",
    "    if is_terminal:\n",
    "        state = preproc.process(env.reset())\n",
    "    else:\n",
    "        state = next_state\n",
    "        \n",
    "    keeper.record_env_step(reward, is_terminal)\n",
    "    \n",
    "    if keeper.need_train:  # TODO train condition call back\n",
    "        loss = trainer.step()\n",
    "        keeper.record_train_step(loss)\n",
    "\n",
    "    if keeper.need_save:\n",
    "        keeper.save()\n",
    "\n",
    "    if keeper.need_draw:\n",
    "        env.render()\n",
    "    \n",
    "    keeper.report_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Framework-F1-END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xdeploy\n",
    "srcs = xdeploy.collect_source_code(THIS_NOTEBOOK, 'F1', RL_components)\n",
    "with open(os.path.join(running_dir, 'run.py'), 'w') as f:\n",
    "    for l in srcs:\n",
    "        f.write(l)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tf]",
   "language": "python",
   "name": "Python [tf]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
