{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var nb = IPython.notebook;\n",
       "var kernel = IPython.notebook.kernel;\n",
       "var command = \"FULL_NOTEBOOK = '\" + nb.base_url + nb.notebook_path + \"'\";\n",
       "kernel.execute(command);"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "var nb = IPython.notebook;\n",
    "var kernel = IPython.notebook.kernel;\n",
    "var command = \"FULL_NOTEBOOK = '\" + nb.base_url + nb.notebook_path + \"'\";\n",
    "kernel.execute(command);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Notebook: nb01a_test_import.ipynb\n"
     ]
    }
   ],
   "source": [
    "THIS_NOTEBOOK = FULL_NOTEBOOK.split('/')[-1]\n",
    "print \"Current Notebook: {}\".format(THIS_NOTEBOOK) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import imp  # Python 2\n",
    "from collections import deque\n",
    "import gym\n",
    "import time\n",
    "import subprocess\n",
    "from dplay_utils.tensordata import to_tensor_f32\n",
    "p = subprocess.Popen('hostname', shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "hostname = p.stdout.readlines()[0][:-1]\n",
    "global USE_CUDA\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "if hostname == 'maibu':\n",
    "    REL_PROJ_PATH = 'local/projects/dplay'\n",
    "else:   # configure project folder on different machines\n",
    "    REL_PROJ_PATH = 'projects/dplay'\n",
    "    \n",
    "FULL_PROJ_PATH = os.path.join(os.environ['HOME'], REL_PROJ_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites-END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data manage\n",
    "Work with Environment. Preprocess Data and Handle GPU CPU trans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from experience_managers.preprocessors import GymAtariFramePreprocessor_Stacker, GymAtariFramePreprocessor_Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from games.aigym import AtariEnvironment_Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from experience_managers.mem import ExperienceMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test Memory and Environment - 1, Save experience\n",
    "%pylab inline\n",
    "preproc = GymAtariFramePreprocessor_Stacker()\n",
    "env = AtariEnvironment_Pong()\n",
    "mem = ExperienceMemory(500, 0.99)\n",
    "s = preproc(env.reset())\n",
    "ei = 0\n",
    "t = 0\n",
    "while ei<10:\n",
    "    print \"\\rt {}\".format(t),\n",
    "    sys.stdout.flush()\n",
    "    s1, r, term, _ = env.step(1)\n",
    "    s1 = preproc(s1)\n",
    "    mem.add_experience(ei, s, 1, r, term)\n",
    "    \n",
    "    if term:\n",
    "        s = preproc(env.reset())\n",
    "        ei += 1\n",
    "    else:\n",
    "        s = s1\n",
    "    env.render()\n",
    "    t += 1\n",
    "    \n",
    "plot(mem.experience['advantages'])\n",
    "plot(mem.experience['rewards'], 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test Memory and Environment - 2: Sanity check.\n",
    "test_states, test_actions, test_advs = mem.get_training_batch(\n",
    "    episodes=[8,9])\n",
    "\n",
    "print test_states.numpy().shape\n",
    "plot(test_advs.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks\n",
    "The neural network that takes states and produces desired assessments. A network consists of two parts: encoder and decoder:\n",
    "- Encoder: This part of the model is generic -- once the extractor has been learned, it can be adapted to other tasks with difference format of inputs (same number of channels) and it is independent with the task-specific target. See below.\n",
    "- Decoder: It takes the features and procudes the outputs, e.g. in Q-learning the targets are action values, in policy gradient, the targets are next action probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from networks.encoders.conv_encoders import DeepConvEncoder, DummyEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from networks.decoders.policy_decoders import Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from networks.nets import RLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "encoder_opts = {\n",
    "    'input_channels': 3,\n",
    "    'convs': [\n",
    "        {'kernel_size':3, 'conv_kernels': 32, 'pool_factor': 2, 'relu': True},\n",
    "        {'kernel_size':3, 'conv_kernels': 64, 'pool_factor': 2, 'relu': True},\n",
    "    ]\n",
    "}\n",
    "\n",
    "encoder = DeepConvEncoder(encoder_opts)\n",
    "\n",
    "nin = encoder.get_feature_num({'height':preproc.im_height, 'width':preproc.im_width})\n",
    "\n",
    "decoder_opts = {\n",
    "    'input_num': nin,\n",
    "    'fc1_hidden_unit_num': 256,\n",
    "    'output_num':4\n",
    "}\n",
    "\n",
    "decoder = Decoder(decoder_opts)\n",
    "net = RLNet(encoder, decoder)\n",
    "y = net(Variable(test_states, requires_grad=False))\n",
    "yv = y.data.numpy()\n",
    "print yv.shape\n",
    "print yv[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy\n",
    "Policies select an action for a state. The state is given in a preprocessed form that is ready to be taken by an RLNet object, which produces assessment of the state. Policy then chooses an action accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from policies.sa_policies import Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sanity check of policy\n",
    "po = Policy(net)\n",
    "s_ = preproc(env.reset())\n",
    "print po.get_action(s_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "Trainer takes recent experience, adjust model parameters to minimise a loss. Hopefully, a smaller loss will lead to a better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rl.train import OneStepPolicyGradientTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sanity check of trainer.\n",
    "opts_ = {'Optimiser': torch.optim.Adagrad, 'learning_rate':1e-6}\n",
    "trainer = OneStepPolicyGradientTrainer(net, mem, opts_)\n",
    "trainer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLLoss is defined as \n",
    "$$\n",
    "\\sum_n - \\log P_{n_i}\n",
    "$$\n",
    "where $n_i$ is the actual class for the $n$-th sample and $P$ is the predicted prob. To minimise the negative value of the log-probability is to push the network so the probability of the classes tha actually happen increases. I.e. when $n_i$-th class is the case for $n$-th sample, you'd like the model to predict more chance of class $n_i$ for the $n$-th sample next time. \n",
    "\n",
    "In RL, we introduce the concept of {\\em advantage}: instead of increasing the likelihood of acutal action, we allow the probability to go both ways -- it get increased if the chosen action turns out to be a good one, and on the contrary, for decisions turns to be bad, it can decrease its future probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeper\n",
    "A Keeper maintains information about the training, such as how many epoches, episodes, minibatches. The methods can be thought as **callbacks** -- to be invoked by the learning algorithm at various occasions, such as when and how to save/load models, when to stop and when to perform evaluation etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rl.keeppg import Keeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sanity check of Keeper\n",
    "# 1. setup\n",
    "preproc = GymAtariFramePreprocessor_Stacker()\n",
    "env = AtariEnvironment_Pong()\n",
    "mem = ExperienceMemory(500, 0.99)\n",
    "encoder_opts = {\n",
    "    'input_channels': 3,\n",
    "    'convs': [\n",
    "        {'kernel_size':3, 'conv_kernels': 32, 'pool_factor': 2, 'relu': True},\n",
    "        {'kernel_size':3, 'conv_kernels': 64, 'pool_factor': 2, 'relu': True},\n",
    "    ]\n",
    "}\n",
    "\n",
    "encoder = DeepConvEncoder(encoder_opts)\n",
    "\n",
    "nin = encoder.get_feature_num({'height':preproc.im_height, 'width':preproc.im_width})\n",
    "\n",
    "decoder_opts = {\n",
    "    'input_num': nin,\n",
    "    'fc1_hidden_unit_num': 256,\n",
    "    'output_num':4\n",
    "}\n",
    "\n",
    "decoder = Decoder(decoder_opts)\n",
    "net = RLNet(encoder, decoder)\n",
    "\n",
    "po = Policy(net)\n",
    "\n",
    "trainer = OneStepPolicyGradientTrainer(net, mem, \n",
    "    {'Optimiser': torch.optim.Adagrad, 'learning_rate':1e-6})\n",
    "\n",
    "path_opts = {\n",
    "    'BASE_PATH': '/Users/junli/local/projects/dplay',\n",
    "    'RUN_PATH': 'RUNS',\n",
    "    'experiment_id': 'TEST01_sanitychk'}\n",
    "\n",
    "running_dir = os.path.join(path_opts['BASE_PATH'], \n",
    "                           path_opts['RUN_PATH'], \n",
    "                           path_opts['experiment_id'])\n",
    "\n",
    "save_dir = os.path.join(running_dir, 'checkpoints')\n",
    "\n",
    "if not os.path.exists(running_dir):\n",
    "    os.mkdir(running_dir)  # NOT using makedirs, I want the \n",
    "    # users to be responsible for the parent directory (and \n",
    "    # overall structure)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "keeper = Keeper([encoder, decoder, po, mem], {\n",
    "    'train_every_n_episodes': 1,\n",
    "    'save_every_n_training_steps': 10,\n",
    "    'draw_every_n_training_steps': -1,\n",
    "    'max_training_steps': 100,\n",
    "    'save_path': save_dir,\n",
    "    'report': {\n",
    "                'save_checkpoint': True,\n",
    "                'every_n_steps': 1,\n",
    "                'every_n_training': 1,\n",
    "                'every_n_episodes': 1, \n",
    "                'every_n_time_records': 100}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. get some data for training\n",
    "state = preproc.process(env.reset())\n",
    "\n",
    "while not keeper.need_train:\n",
    "    action, action_prob = po.get_action(state)\n",
    "    next_state, reward, is_terminal, _ = env.step(action)\n",
    "    next_state = preproc.process(next_state)\n",
    "    ep = keeper.records['episodes']\n",
    "    mem.add_experience(ep, state, action, reward, is_terminal, None)\n",
    "    keeper.record_env_step(reward, is_terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3. do training\n",
    "for i in range(10):\n",
    "    loss = trainer.step()\n",
    "    keeper.record_train_step(loss)\n",
    "    print i, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4. save and load\n",
    "keeper.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GO ABOVE, re-initialise the encoder/decoder / re-collect experience, see if\n",
    "# the training starts from where it is supposed to \n",
    "keeper.load()\n",
    "for i in range(10):\n",
    "    loss = trainer.step()\n",
    "    keeper.record_train_step(loss)\n",
    "    print i, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keeper.load()\n",
    "state = preproc.process(env.reset())\n",
    "while not keeper.need_stop:\n",
    "    action, action_prob = po.get_action(state)\n",
    "    next_state, reward, is_terminal, _ = env.step(action)\n",
    "    next_state = preproc.process(next_state)\n",
    "    mem.add_experience(state, action, reward, is_terminal, None)\n",
    "    if is_term:\n",
    "        state = preproc.process(env.reset())\n",
    "    else:\n",
    "        state = next_state\n",
    "    \n",
    "    keeper.record_env_step(reward, is_terminal)\n",
    "    \n",
    "    if keeper.need_train:  # TODO train condition call back\n",
    "        loss = train_step()\n",
    "        keeper.record_train_step(loss)\n",
    "        \n",
    "    if keeper.need_save:\n",
    "        keeper.save()\n",
    "        \n",
    "    if keeper.need_draw:\n",
    "        env.render()\n",
    "        \n",
    "    keeper.report_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components-END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framework-F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Framework definition:\n",
    "# **Necessary to run this cell** to create experiment package for this framework\n",
    "RL_components = {\n",
    "    'Preprocessor': GymAtariFramePreprocessor_Stacker,\n",
    "    'ExperienceMemoryManager': ExperienceMemory,\n",
    "    'Encoder': DeepConvEncoder,\n",
    "    'Decoder': Decoder,\n",
    "    'RLNet': RLNet,\n",
    "    'Policy': Policy,\n",
    "    'Environment': AtariEnvironment_Pong,\n",
    "    'Trainer': OneStepPolicyGradientTrainer,\n",
    "    'Keeper': Keeper,\n",
    "}\n",
    "\n",
    "experience_opts = {\n",
    "    'capacity': 1000,\n",
    "    'discount': 0.99\n",
    "}\n",
    "\n",
    "encoder_opts = {\n",
    "    'input_channels': 3,\n",
    "    'convs': [\n",
    "        {'kernel_size':3, 'conv_kernels': 32, 'pool_factor': 2, 'relu': True},\n",
    "        {'kernel_size':3, 'conv_kernels': 64, 'pool_factor': 2, 'relu': True},\n",
    "    ]\n",
    "}\n",
    "\n",
    "decoder_opts = {\n",
    "    'input_num': None,\n",
    "    'fc1_hidden_unit_num': 256,\n",
    "    'output_num':4\n",
    "}\n",
    "\n",
    "trainer_opts = {'Optimiser': torch.optim.Adagrad, 'learning_rate':1e-4}\n",
    "\n",
    "path_opts = {\n",
    "    'BASE_PATH': FULL_PROJ_PATH,\n",
    "    'RUN_PATH': 'RUNS',\n",
    "    'experiment_id': 'TEST01a'}\n",
    "\n",
    "running_dir = os.path.join(path_opts['BASE_PATH'], \n",
    "                           path_opts['RUN_PATH'], \n",
    "                           path_opts['experiment_id'])\n",
    "\n",
    "save_dir = os.path.join(running_dir, 'checkpoints')\n",
    "\n",
    "if not os.path.exists(running_dir):\n",
    "    os.mkdir(running_dir)  # NOT using makedirs, I want the \n",
    "    # users to be responsible for the parent directory (and \n",
    "    # overall structure)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "keeper_opts = {\n",
    "    'train_every_n_episodes': 1,\n",
    "    'save_every_n_training_steps': 5000,\n",
    "    'draw_every_n_training_steps': -1,\n",
    "    'max_training_steps': 2000000,\n",
    "    'save_path': save_dir,\n",
    "    'report': {'save_checkpoint': True,\n",
    "               'every_n_steps': -1,\n",
    "               'every_n_training': 1,\n",
    "               'every_n_episodes': 1,\n",
    "               'every_n_time_records': 100}\n",
    "}\n",
    "    \n",
    "# CREATE LEARNING COMPONENTS\n",
    "env = RL_components['Environment']()\n",
    "preproc = RL_components['Preprocessor']()\n",
    "mem = RL_components['ExperienceMemoryManager'](**experience_opts)\n",
    "enc = RL_components['Encoder'](encoder_opts)\n",
    "decoder_opts['input_num'] = enc.get_feature_num({'height':preproc.im_height, 'width':preproc.im_width})\n",
    "dec = RL_components['Decoder'](decoder_opts)\n",
    "rlnet = RL_components['RLNet'](enc, dec)\n",
    "policy = RL_components['Policy'](rlnet)\n",
    "trainer = RL_components['Trainer'](rlnet, mem, trainer_opts)\n",
    "keeper = RL_components['Keeper']([enc, dec, policy, mem], keeper_opts)  # objects has \"save/load\" interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 steps  87 reward -1.0 running episode reward -1.00  Training step 1, loss -0.126, running_loss -0.126 \n",
      "Episode 2 steps  83 reward -1.0 running episode reward -1.00  Training step 2, loss 0.008, running_loss -0.125 \n",
      "Episode 3 steps  85 reward -1.0 running episode reward -1.00  Training step 3, loss 0.030, running_loss -0.123 \n",
      "Episode 4 steps 165 reward -1.0 running episode reward -1.00  Training step 4, loss -0.281, running_loss -0.125 \n",
      "Episode 5 steps  83 reward -1.0 running episode reward -1.00  Training step 5, loss -0.099, running_loss -0.125 \n",
      "Episode 6 steps  82 reward -1.0 running episode reward -1.00  Training step 6, loss 0.038, running_loss -0.123 \n",
      "Episode 7 steps  84 reward -1.0 running episode reward -1.00  Training step 7, loss -0.002, running_loss -0.122 \n",
      "Episode 8 steps  85 reward -1.0 running episode reward -1.00  Training step 8, loss 0.046, running_loss -0.120 \n",
      "Episode 9 steps  89 reward -1.0 running episode reward -1.00  Training step 9, loss 0.172, running_loss -0.117 \n",
      "Episode 10 steps  83 reward -1.0 running episode reward -1.00  Training step 10, loss 0.017, running_loss -0.116 \n",
      "Episode 11 steps  87 reward -1.0 running episode reward -1.00  Training step 11, loss 0.112, running_loss -0.113 \n",
      "Episode 12 steps 170 reward -1.0 running episode reward -1.00  Training step 12, loss -0.039, running_loss -0.113 \n",
      "Episode 13 steps  85 reward -1.0 running episode reward -1.00  Training step 13, loss 0.025, running_loss -0.111 \n",
      "Episode 14 steps  89 reward -1.0 running episode reward -1.00  Training step 14, loss -0.010, running_loss -0.110 \n",
      "Episode 15 steps  82 reward -1.0 running episode reward -1.00  Training step 15, loss -0.095, running_loss -0.110 \n",
      "Episode 16 steps  86 reward -1.0 running episode reward -1.00  Training step 16, loss 0.011, running_loss -0.109 \n",
      "Episode 17 steps  86 reward -1.0 running episode reward -1.00  Training step 17, loss 0.009, running_loss -0.108 \n",
      "Episode 18 steps  83 reward -1.0 running episode reward -1.00  Training step 18, loss 0.013, running_loss -0.107 \n",
      "Episode 19 steps 129 reward 1.0 running episode reward -0.98  Training step 19, loss -0.221, running_loss -0.108 \n",
      "Episode 20 steps 160 reward -1.0 running episode reward -0.98  Training step 20, loss -0.115, running_loss -0.108 \n",
      "Episode 21 steps 165 reward -1.0 running episode reward -0.98  Training step 21, loss 0.071, running_loss -0.106 \n",
      "Episode 22 steps 129 reward 1.0 running episode reward -0.96  Training step 22, loss 0.190, running_loss -0.103 \n",
      "Episode 23 steps  89 reward -1.0 running episode reward -0.96  Training step 23, loss -0.182, running_loss -0.104 \n",
      "Episode 24 steps  78 reward -1.0 running episode reward -0.96  Training step 24, loss 0.114, running_loss -0.102 \n",
      "Episode 25 steps  86 reward -1.0 running episode reward -0.96  Training step 25, loss -0.035, running_loss -0.101 \n",
      "Episode 26 steps  88 reward -1.0 running episode reward -0.96  Training step 26, loss -0.065, running_loss -0.101 \n",
      "Episode 27 steps  86 reward -1.0 running episode reward -0.96  Training step 27, loss -0.074, running_loss -0.100 \n",
      "Episode 28 steps  83 reward -1.0 running episode reward -0.96  Training step 28, loss -0.116, running_loss -0.101 \n",
      "Episode 29 steps  83 reward -1.0 running episode reward -0.96  Training step 29, loss -0.021, running_loss -0.100 \n",
      "Episode 30 steps  90 reward -1.0 running episode reward -0.96  Training step 30, loss 0.028, running_loss -0.098 \n",
      "Episode 31 steps  89 reward -1.0 running episode reward -0.96  Training step 31, loss 0.041, running_loss -0.097 \n",
      "Episode 32 steps 127 reward 1.0 running episode reward -0.94  Training step 32, loss -0.122, running_loss -0.097 \n",
      "Episode 33 steps  84 reward -1.0 running episode reward -0.94  Training step 33, loss -0.054, running_loss -0.097 \n",
      "Episode 34 steps  87 reward -1.0 running episode reward -0.95  Training step 34, loss -0.063, running_loss -0.097 \n",
      "Episode 35 steps  89 reward -1.0 running episode reward -0.95  Training step 35, loss 0.085, running_loss -0.095 \n",
      "Episode 36 steps  88 reward -1.0 running episode reward -0.95  Training step 36, loss 0.259, running_loss -0.091 \n",
      "Episode 37 steps  83 reward -1.0 running episode reward -0.95  Training step 37, loss 0.031, running_loss -0.090 \n",
      "Episode 38 steps  88 reward -1.0 running episode reward -0.95  Training step 38, loss -0.028, running_loss -0.089 \n",
      "Episode 39 steps  88 reward -1.0 running episode reward -0.95  Training step 39, loss 0.062, running_loss -0.088 \n",
      "Episode 40 steps  81 reward -1.0 running episode reward -0.95  Training step 40, loss 0.029, running_loss -0.087 \n",
      "Episode 41 steps  83 reward -1.0 running episode reward -0.95  Training step 41, loss -0.107, running_loss -0.087 \n",
      "Episode 42 steps  84 reward -1.0 running episode reward -0.95  Training step 42, loss 0.051, running_loss -0.086 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a338211da190>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mkeeper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'policy.get_action'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_terminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mkeeper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'env.step'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junli/local/projects/dplay/sandbox/ExpFramework/games/aigym.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, act)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junli/toolbox/anaconda2/envs/tf/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \"\"\"\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junli/toolbox/anaconda2/envs/tf/lib/python2.7/site-packages/gym/wrappers/time_limit.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junli/toolbox/anaconda2/envs/tf/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \"\"\"\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junli/toolbox/anaconda2/envs/tf/lib/python2.7/site-packages/gym/envs/atari/atari_env.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/junli/toolbox/anaconda2/envs/tf/lib/python2.7/site-packages/atari_py/ale_python_interface.pyc\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# RUNNING: this part does the actual work. \n",
    "# NOT necessary to run this cell to create experiment package for this framework\n",
    "keeper.load()\n",
    "state = preproc.process(env.reset())\n",
    "\n",
    "\n",
    "while not keeper.need_stop:\n",
    "    keeper.set_timer()\n",
    "    action, action_prob = policy.get_action(state)\n",
    "    keeper.record_time('policy.get_action')\n",
    "    next_state, reward, is_terminal, _ = env.step(action)\n",
    "    keeper.record_time('env.step')\n",
    "    next_state = preproc(next_state)\n",
    "    ep = keeper.records['episodes']\n",
    "    keeper.set_timer()\n",
    "    mem.add_experience(ep, state, action, reward, is_terminal, None)\n",
    "    keeper.record_time('mem.add_experience')\n",
    "    # None: We don't use last prediction (will predict in traing step)\n",
    "    \n",
    "    if is_terminal:\n",
    "        state = preproc.process(env.reset())\n",
    "    else:\n",
    "        state = next_state\n",
    "        \n",
    "    keeper.set_timer()\n",
    "    keeper.record_env_step(reward, is_terminal)\n",
    "    keeper.record_time('record_env_step')\n",
    "    \n",
    "    if keeper.need_train:  # TODO train condition call back\n",
    "        loss = trainer.step()\n",
    "        keeper.record_time('trainer.step')\n",
    "        keeper.record_train_step(loss)\n",
    "        keeper.record_time('record_train_step')\n",
    "\n",
    "    if keeper.need_save:\n",
    "        keeper.save()\n",
    "\n",
    "    if keeper.need_draw:\n",
    "        env.render()\n",
    "    \n",
    "    keeper.set_timer()\n",
    "    keeper.report_step()\n",
    "    keeper.record_time('report_step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Framework-F1-END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framework-DiffPreproc1\n",
    "Using the differecen between two consecutive frames as input -- easy and it worked.\n",
    "Using easy encoder design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-16 16:46:56,280] Making new env: Pong-v0\n"
     ]
    }
   ],
   "source": [
    "# Framework definition:\n",
    "# **Necessary to run this cell** to create experiment package for this framework\n",
    "\n",
    "RL_components = {\n",
    "    'Preprocessor': GymAtariFramePreprocessor_Diff,\n",
    "    'ExperienceMemoryManager': ExperienceMemory,\n",
    "    'Encoder': DummyEncoder,\n",
    "    'Decoder': Decoder,\n",
    "    'RLNet': RLNet,\n",
    "    'Policy': Policy,\n",
    "    'Environment': AtariEnvironment_Pong,\n",
    "    'Trainer': OneStepPolicyGradientTrainer,\n",
    "    'Keeper': Keeper,\n",
    "}\n",
    "\n",
    "experience_opts = {\n",
    "    'capacity': 1000,\n",
    "    'discount': 0.99\n",
    "}\n",
    "\n",
    "encoder_opts = {\n",
    "    'input_channels': 1,\n",
    "}\n",
    "\n",
    "decoder_opts = {\n",
    "    'input_num': None,\n",
    "    'fc1_hidden_unit_num': 256,\n",
    "    'output_num':4\n",
    "}\n",
    "\n",
    "trainer_opts = {'Optimiser': torch.optim.Adagrad, 'learning_rate':1e-4}\n",
    "\n",
    "path_opts = {\n",
    "    'BASE_PATH': FULL_PROJ_PATH,\n",
    "    'RUN_PATH': 'RUNS',\n",
    "    'experiment_id': 'DiffPreproc1'}\n",
    "\n",
    "running_dir = os.path.join(path_opts['BASE_PATH'], \n",
    "                           path_opts['RUN_PATH'], \n",
    "                           path_opts['experiment_id'])\n",
    "\n",
    "save_dir = os.path.join(running_dir, 'checkpoints')\n",
    "\n",
    "if not os.path.exists(running_dir):\n",
    "    os.mkdir(running_dir)  # NOT using makedirs, I want the \n",
    "    # users to be responsible for the parent directory (and \n",
    "    # overall structure)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "keeper_opts = {\n",
    "    'train_every_n_episodes': 1,\n",
    "    'save_every_n_training_steps': 5000,\n",
    "    'draw_every_n_training_steps': -1,\n",
    "    'max_training_steps': 2000000,\n",
    "    'save_path': save_dir,\n",
    "    'report': {'save_checkpoint': True,\n",
    "               'every_n_steps': -1,\n",
    "               'every_n_training': 1,\n",
    "               'every_n_episodes': 1,\n",
    "               'every_n_time_records': 100}\n",
    "}\n",
    "    \n",
    "# CREATE LEARNING COMPONENTS\n",
    "env = RL_components['Environment']()\n",
    "preproc = RL_components['Preprocessor']()\n",
    "mem = RL_components['ExperienceMemoryManager'](**experience_opts)\n",
    "enc = RL_components['Encoder'](encoder_opts)\n",
    "decoder_opts['input_num'] = enc.get_feature_num({'height':preproc.im_height, 'width':preproc.im_width})\n",
    "dec = RL_components['Decoder'](decoder_opts)\n",
    "rlnet = RL_components['RLNet'](enc, dec)\n",
    "policy = RL_components['Policy'](rlnet)\n",
    "trainer = RL_components['Trainer'](rlnet, mem, trainer_opts)\n",
    "keeper = RL_components['Keeper']([enc, dec, policy, mem], keeper_opts)  # objects has \"save/load\" interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RUNNING: this part does the actual work. \n",
    "# NOT necessary to run this cell to create experiment package for this framework\n",
    "keeper.load()\n",
    "state = preproc.process(env.reset())\n",
    "\n",
    "\n",
    "while not keeper.need_stop:\n",
    "    keeper.set_timer()\n",
    "    action, action_prob = policy.get_action(state)\n",
    "    keeper.record_time('policy.get_action')\n",
    "    next_state, reward, is_terminal, _ = env.step(action)\n",
    "    keeper.record_time('env.step')\n",
    "    next_state = preproc(next_state)\n",
    "    ep = keeper.records['episodes']\n",
    "    keeper.set_timer()\n",
    "    mem.add_experience(ep, state, action, reward, is_terminal, None)\n",
    "    keeper.record_time('mem.add_experience')\n",
    "    # None: We don't use last prediction (will predict in traing step)\n",
    "    \n",
    "    if is_terminal:\n",
    "        state = preproc.process(env.reset())\n",
    "    else:\n",
    "        state = next_state\n",
    "        \n",
    "    keeper.set_timer()\n",
    "    keeper.record_env_step(reward, is_terminal)\n",
    "    keeper.record_time('record_env_step')\n",
    "    \n",
    "    if keeper.need_train:  # TODO train condition call back\n",
    "        loss = trainer.step()\n",
    "        keeper.record_time('trainer.step')\n",
    "        keeper.record_train_step(loss)\n",
    "        keeper.record_time('record_train_step')\n",
    "\n",
    "    if keeper.need_save:\n",
    "        keeper.save()\n",
    "\n",
    "    if keeper.need_draw:\n",
    "        env.render()\n",
    "    \n",
    "    keeper.set_timer()\n",
    "    keeper.report_step()\n",
    "    keeper.record_time('report_step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framework-DiffPreproc1-END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dplay_utils.xdeploy as xd\n",
    "xd.deploy(THIS_NOTEBOOK, 'DiffPreproc1', RL_components, '../../RUNS/TMPTEST2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['from', 'a.e.f', 'import', 'b', 'c']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('[\\s,]+', 'from a.e.f import b, c')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tf]",
   "language": "python",
   "name": "Python [tf]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
