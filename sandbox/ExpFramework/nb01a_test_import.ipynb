{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "var nb = IPython.notebook;\n",
    "var kernel = IPython.notebook.kernel;\n",
    "var command = \"FULL_NOTEBOOK = '\" + nb.base_url + nb.notebook_path + \"'\";\n",
    "kernel.execute(command);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THIS_NOTEBOOK = FULL_NOTEBOOK.split('/')[-1]\n",
    "print \"Current Notebook: {}\".format(THIS_NOTEBOOK) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import imp  # Python 2\n",
    "from collections import deque\n",
    "import gym\n",
    "import time\n",
    "import subprocess\n",
    "from dplay_utils.tensordata import to_tensor_f32\n",
    "p = subprocess.Popen('hostname', shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "hostname = p.stdout.readlines()[0][:-1]\n",
    "global USE_CUDA\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "if hostname == 'maibu':\n",
    "    REL_PROJ_PATH = 'local/projects/dplay'\n",
    "else:   # configure project folder on different machines\n",
    "    REL_PROJ_PATH = 'projects/dplay'\n",
    "    \n",
    "FULL_PROJ_PATH = os.path.join(os.environ['HOME'], REL_PROJ_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites-END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data manage\n",
    "Work with Environment. Preprocess Data and Handle GPU CPU trans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from experience_managers.preprocessors import GymAtariFramePreprocessor_Stacker, GymAtariFramePreprocessor_Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from games.aigym import AtariEnvironment_Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from experience_managers.mem import ExperienceMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Memory and Environment - 1, Save experience\n",
    "%pylab inline\n",
    "preproc = GymAtariFramePreprocessor_Stacker()\n",
    "env = AtariEnvironment_Pong()\n",
    "mem = ExperienceMemory(500, 0.99)\n",
    "s = preproc(env.reset())\n",
    "ei = 0\n",
    "t = 0\n",
    "while ei<10:\n",
    "    print \"\\rt {}\".format(t),\n",
    "    sys.stdout.flush()\n",
    "    s1, r, term, _ = env.step(1)\n",
    "    s1 = preproc(s1)\n",
    "    mem.add_experience(ei, s, 1, r, term)\n",
    "    \n",
    "    if term:\n",
    "        s = preproc(env.reset())\n",
    "        ei += 1\n",
    "    else:\n",
    "        s = s1\n",
    "    env.render()\n",
    "    t += 1\n",
    "    \n",
    "plot(mem.experience['advantages'])\n",
    "plot(mem.experience['rewards'], 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Memory and Environment - 2: Sanity check.\n",
    "test_states, test_actions, test_advs = mem.get_training_batch(\n",
    "    episodes=[8,9])\n",
    "\n",
    "print test_states.numpy().shape\n",
    "plot(test_advs.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks\n",
    "The neural network that takes states and produces desired assessments. A network consists of two parts: encoder and decoder:\n",
    "- Encoder: This part of the model is generic -- once the extractor has been learned, it can be adapted to other tasks with difference format of inputs (same number of channels) and it is independent with the task-specific target. See below.\n",
    "- Decoder: It takes the features and procudes the outputs, e.g. in Q-learning the targets are action values, in policy gradient, the targets are next action probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from networks.encoders.conv_encoders import DeepConvEncoder, DummyEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from networks.decoders.policy_decoders import Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from networks.nets import RLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "encoder_opts = {\n",
    "    'input_channels': 3,\n",
    "    'convs': [\n",
    "        {'kernel_size':3, 'conv_kernels': 32, 'pool_factor': 2, 'relu': True},\n",
    "        {'kernel_size':3, 'conv_kernels': 64, 'pool_factor': 2, 'relu': True},\n",
    "    ]\n",
    "}\n",
    "\n",
    "encoder = DeepConvEncoder(encoder_opts)\n",
    "\n",
    "nin = encoder.get_feature_num({'height':preproc.im_height, 'width':preproc.im_width})\n",
    "\n",
    "decoder_opts = {\n",
    "    'input_num': nin,\n",
    "    'fc1_hidden_unit_num': 256,\n",
    "    'output_num':4\n",
    "}\n",
    "\n",
    "decoder = Decoder(decoder_opts)\n",
    "net = RLNet(encoder, decoder)\n",
    "y = net(Variable(test_states, requires_grad=False))\n",
    "yv = y.data.numpy()\n",
    "print yv.shape\n",
    "print yv[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy\n",
    "Policies select an action for a state. The state is given in a preprocessed form that is ready to be taken by an RLNet object, which produces assessment of the state. Policy then chooses an action accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from policies.sa_policies import Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check of policy\n",
    "po = Policy(net)\n",
    "s_ = preproc(env.reset())\n",
    "print po.get_action(s_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "Trainer takes recent experience, adjust model parameters to minimise a loss. Hopefully, a smaller loss will lead to a better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.train import OneStepPolicyGradientTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check of trainer.\n",
    "opts_ = {'Optimiser': torch.optim.Adagrad, 'learning_rate':1e-6}\n",
    "trainer = OneStepPolicyGradientTrainer(net, mem, opts_)\n",
    "trainer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLLoss is defined as \n",
    "$$\n",
    "\\sum_n - \\log P_{n_i}\n",
    "$$\n",
    "where $n_i$ is the actual class for the $n$-th sample and $P$ is the predicted prob. To minimise the negative value of the log-probability is to push the network so the probability of the classes tha actually happen increases. I.e. when $n_i$-th class is the case for $n$-th sample, you'd like the model to predict more chance of class $n_i$ for the $n$-th sample next time. \n",
    "\n",
    "In RL, we introduce the concept of {\\em advantage}: instead of increasing the likelihood of acutal action, we allow the probability to go both ways -- it get increased if the chosen action turns out to be a good one, and on the contrary, for decisions turns to be bad, it can decrease its future probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeper\n",
    "A Keeper maintains information about the training, such as how many epoches, episodes, minibatches. The methods can be thought as **callbacks** -- to be invoked by the learning algorithm at various occasions, such as when and how to save/load models, when to stop and when to perform evaluation etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rl.keeppg import Keeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check of Keeper\n",
    "# 1. setup\n",
    "preproc = GymAtariFramePreprocessor_Stacker()\n",
    "env = AtariEnvironment_Pong()\n",
    "mem = ExperienceMemory(500, 0.99)\n",
    "encoder_opts = {\n",
    "    'input_channels': 3,\n",
    "    'convs': [\n",
    "        {'kernel_size':3, 'conv_kernels': 32, 'pool_factor': 2, 'relu': True},\n",
    "        {'kernel_size':3, 'conv_kernels': 64, 'pool_factor': 2, 'relu': True},\n",
    "    ]\n",
    "}\n",
    "\n",
    "encoder = DeepConvEncoder(encoder_opts)\n",
    "\n",
    "nin = encoder.get_feature_num({'height':preproc.im_height, 'width':preproc.im_width})\n",
    "\n",
    "decoder_opts = {\n",
    "    'input_num': nin,\n",
    "    'fc1_hidden_unit_num': 256,\n",
    "    'output_num':4\n",
    "}\n",
    "\n",
    "decoder = Decoder(decoder_opts)\n",
    "net = RLNet(encoder, decoder)\n",
    "\n",
    "po = Policy(net)\n",
    "\n",
    "trainer = OneStepPolicyGradientTrainer(net, mem, \n",
    "    {'Optimiser': torch.optim.Adagrad, 'learning_rate':1e-4})\n",
    "\n",
    "path_opts = {\n",
    "    'BASE_PATH': '/Users/junli/local/projects/dplay',\n",
    "    'RUN_PATH': 'RUNS',\n",
    "    'experiment_id': 'TEST01_sanitychk'}\n",
    "\n",
    "running_dir = os.path.join(path_opts['BASE_PATH'], \n",
    "                           path_opts['RUN_PATH'], \n",
    "                           path_opts['experiment_id'])\n",
    "\n",
    "save_dir = os.path.join(running_dir, 'checkpoints')\n",
    "\n",
    "if not os.path.exists(running_dir):\n",
    "    os.mkdir(running_dir)  # NOT using makedirs, I want the \n",
    "    # users to be responsible for the parent directory (and \n",
    "    # overall structure)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "keeper = Keeper([encoder, decoder, po, mem], {\n",
    "    'train_every_n_episodes': 1,\n",
    "    'save_every_n_training_steps': 10,\n",
    "    'draw_every_n_training_steps': -1,\n",
    "    'max_training_steps': 100,\n",
    "    'save_path': save_dir,\n",
    "    'report': {\n",
    "                'save_checkpoint': True,\n",
    "                'every_n_steps': 1,\n",
    "                'every_n_training': 1,\n",
    "                'every_n_episodes': 1, \n",
    "                'every_n_time_records': 100}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. get some data for training\n",
    "state = preproc.process(env.reset())\n",
    "\n",
    "while not keeper.need_train:\n",
    "    action, action_prob = po.get_action(state)\n",
    "    next_state, reward, is_terminal, _ = env.step(action)\n",
    "    next_state = preproc.process(next_state)\n",
    "    ep = keeper.records['episodes']\n",
    "    mem.add_experience(ep, state, action, reward, is_terminal, None)\n",
    "    keeper.record_env_step(reward, is_terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. do training\n",
    "for i in range(10):\n",
    "    loss = trainer.step()\n",
    "    keeper.record_train_step(loss)\n",
    "    print i, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. save and load\n",
    "keeper.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GO ABOVE, re-initialise the encoder/decoder / re-collect experience, see if\n",
    "# the training starts from where it is supposed to \n",
    "keeper.load()\n",
    "for i in range(10):\n",
    "    loss = trainer.step()\n",
    "    keeper.record_train_step(loss)\n",
    "    print i, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keeper.load()\n",
    "state = preproc.process(env.reset())\n",
    "while not keeper.need_stop:\n",
    "    action, action_prob = po.get_action(state)\n",
    "    next_state, reward, is_terminal, _ = env.step(action)\n",
    "    next_state = preproc.process(next_state)\n",
    "    mem.add_experience(state, action, reward, is_terminal, None)\n",
    "    if is_term:\n",
    "        state = preproc.process(env.reset())\n",
    "    else:\n",
    "        state = next_state\n",
    "    \n",
    "    keeper.record_env_step(reward, is_terminal)\n",
    "    \n",
    "    if keeper.need_train:  # TODO train condition call back\n",
    "        loss = train_step()\n",
    "        keeper.record_train_step(loss)\n",
    "        \n",
    "    if keeper.need_save:\n",
    "        keeper.save()\n",
    "        \n",
    "    if keeper.need_draw:\n",
    "        env.render()\n",
    "        \n",
    "    keeper.report_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components-END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framework-F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framework definition:\n",
    "# **Necessary to run this cell** to create experiment package for this framework\n",
    "RL_components = {\n",
    "    'Preprocessor': GymAtariFramePreprocessor_Stacker,\n",
    "    'ExperienceMemoryManager': ExperienceMemory,\n",
    "    'Encoder': DeepConvEncoder,\n",
    "    'Decoder': Decoder,\n",
    "    'RLNet': RLNet,\n",
    "    'Policy': Policy,\n",
    "    'Environment': AtariEnvironment_Pong,\n",
    "    'Trainer': OneStepPolicyGradientTrainer,\n",
    "    'Keeper': Keeper,\n",
    "}\n",
    "\n",
    "experience_opts = {\n",
    "    'capacity': 1000,\n",
    "    'discount': 0.99\n",
    "}\n",
    "\n",
    "encoder_opts = {\n",
    "    'input_channels': 3,\n",
    "    'convs': [\n",
    "        {'kernel_size':3, 'conv_kernels': 32, 'pool_factor': 2, 'relu': True},\n",
    "        {'kernel_size':3, 'conv_kernels': 64, 'pool_factor': 2, 'relu': True},\n",
    "    ]\n",
    "}\n",
    "\n",
    "decoder_opts = {\n",
    "    'input_num': None,\n",
    "    'fc1_hidden_unit_num': 256,\n",
    "    'output_num':4\n",
    "}\n",
    "\n",
    "trainer_opts = {'Optimiser': torch.optim.Adagrad, 'learning_rate':1e-4}\n",
    "\n",
    "path_opts = {\n",
    "    'BASE_PATH': FULL_PROJ_PATH,\n",
    "    'RUN_PATH': 'RUNS',\n",
    "    'experiment_id': 'TEST01a'}\n",
    "\n",
    "running_dir = os.path.join(path_opts['BASE_PATH'], \n",
    "                           path_opts['RUN_PATH'], \n",
    "                           path_opts['experiment_id'])\n",
    "\n",
    "save_dir = os.path.join(running_dir, 'checkpoints')\n",
    "\n",
    "if not os.path.exists(running_dir):\n",
    "    os.mkdir(running_dir)  # NOT using makedirs, I want the \n",
    "    # users to be responsible for the parent directory (and \n",
    "    # overall structure)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "keeper_opts = {\n",
    "    'train_every_n_episodes': 1,\n",
    "    'save_every_n_training_steps': 5000,\n",
    "    'draw_every_n_training_steps': -1,\n",
    "    'max_training_steps': 2000000,\n",
    "    'save_path': save_dir,\n",
    "    'report': {'save_checkpoint': True,\n",
    "               'every_n_steps': -1,\n",
    "               'every_n_training': 1,\n",
    "               'every_n_episodes': 1,\n",
    "               'every_n_time_records': 100}\n",
    "}\n",
    "    \n",
    "# CREATE LEARNING COMPONENTS\n",
    "env = RL_components['Environment']()\n",
    "preproc = RL_components['Preprocessor']()\n",
    "mem = RL_components['ExperienceMemoryManager'](**experience_opts)\n",
    "enc = RL_components['Encoder'](encoder_opts)\n",
    "decoder_opts['input_num'] = enc.get_feature_num({'height':preproc.im_height, 'width':preproc.im_width})\n",
    "dec = RL_components['Decoder'](decoder_opts)\n",
    "rlnet = RL_components['RLNet'](enc, dec)\n",
    "policy = RL_components['Policy'](rlnet)\n",
    "trainer = RL_components['Trainer'](rlnet, mem, trainer_opts)\n",
    "keeper = RL_components['Keeper']([enc, dec, policy, mem], keeper_opts)  # objects has \"save/load\" interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUNNING: this part does the actual work. \n",
    "# NOT necessary to run this cell to create experiment package for this framework\n",
    "keeper.load()\n",
    "state = preproc.process(env.reset())\n",
    "\n",
    "\n",
    "while not keeper.need_stop:\n",
    "    keeper.set_timer()\n",
    "    action, action_prob = policy.get_action(state)\n",
    "    keeper.record_time('policy.get_action')\n",
    "    next_state, reward, is_terminal, _ = env.step(action)\n",
    "    keeper.record_time('env.step')\n",
    "    next_state = preproc(next_state)\n",
    "    ep = keeper.records['episodes']\n",
    "    keeper.set_timer()\n",
    "    mem.add_experience(ep, state, action, reward, is_terminal, None)\n",
    "    keeper.record_time('mem.add_experience')\n",
    "    # None: We don't use last prediction (will predict in traing step)\n",
    "    \n",
    "    if is_terminal:\n",
    "        state = preproc.process(env.reset())\n",
    "    else:\n",
    "        state = next_state\n",
    "        \n",
    "    keeper.set_timer()\n",
    "    keeper.record_env_step(reward, is_terminal)\n",
    "    keeper.record_time('record_env_step')\n",
    "    \n",
    "    if keeper.need_train:  # TODO train condition call back\n",
    "        loss = trainer.step()\n",
    "        keeper.record_time('trainer.step')\n",
    "        keeper.record_train_step(loss)\n",
    "        keeper.record_time('record_train_step')\n",
    "\n",
    "    if keeper.need_save:\n",
    "        keeper.save()\n",
    "\n",
    "    if keeper.need_draw:\n",
    "        env.render()\n",
    "    \n",
    "    keeper.set_timer()\n",
    "    keeper.report_step()\n",
    "    keeper.record_time('report_step')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Framework-F1-END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framework-DiffPreproc1\n",
    "Using the differecen between two consecutive frames as input -- easy and it worked.\n",
    "Using easy encoder design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framework definition:\n",
    "# **Necessary to run this cell** to create experiment package for this framework\n",
    "from rl.keeppg import RLAlgorithm\n",
    "RL_components = {\n",
    "    'Preprocessor': GymAtariFramePreprocessor_Diff,\n",
    "    'ExperienceMemoryManager': ExperienceMemory,\n",
    "    'Encoder': DummyEncoder,\n",
    "    'Decoder': Decoder,\n",
    "    'RLNet': RLNet,\n",
    "    'Policy': Policy,\n",
    "    'Environment': AtariEnvironment_Pong,\n",
    "    'Trainer': OneStepPolicyGradientTrainer,\n",
    "    'Keeper': Keeper,\n",
    "}\n",
    "\n",
    "experience_opts = {\n",
    "    'capacity': 1000,\n",
    "    'discount': 0.99\n",
    "}\n",
    "\n",
    "encoder_opts = {\n",
    "    'input_channels': 1,\n",
    "}\n",
    "\n",
    "decoder_opts = {\n",
    "    'input_num': None,\n",
    "    'fc1_hidden_unit_num': 256,\n",
    "    'output_num':4\n",
    "}\n",
    "\n",
    "trainer_opts = {'Optimiser': torch.optim.Adagrad, 'learning_rate':1e-4}\n",
    "\n",
    "path_opts = {\n",
    "    'BASE_PATH': FULL_PROJ_PATH,\n",
    "    'RUN_PATH': 'RUNS',\n",
    "    'experiment_id': 'DiffPreproc1'}\n",
    "\n",
    "running_dir = os.path.join(path_opts['BASE_PATH'], \n",
    "                           path_opts['RUN_PATH'], \n",
    "                           path_opts['experiment_id'])\n",
    "\n",
    "save_dir = os.path.join(running_dir, 'checkpoints')\n",
    "\n",
    "if not os.path.exists(running_dir):\n",
    "    os.mkdir(running_dir)  # NOT using makedirs, I want the \n",
    "    # users to be responsible for the parent directory (and \n",
    "    # overall structure)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "keeper_opts = {\n",
    "    'train_every_n_episodes': 1,\n",
    "    'save_every_n_training_steps': 5000,\n",
    "    'draw_every_n_training_steps': -1,\n",
    "    'max_training_steps': 2000000,\n",
    "    'save_path': save_dir,\n",
    "    'report': {'save_checkpoint': True,\n",
    "               'every_n_steps': 10000,\n",
    "               'every_n_training': 1,\n",
    "               'every_n_episodes': 1,\n",
    "               'every_n_time_records': 100}\n",
    "}\n",
    "    \n",
    "# CREATE LEARNING COMPONENTS\n",
    "env = RL_components['Environment']()\n",
    "preproc = RL_components['Preprocessor']()\n",
    "mem = RL_components['ExperienceMemoryManager'](**experience_opts)\n",
    "enc = RL_components['Encoder'](encoder_opts)\n",
    "decoder_opts['input_num'] = enc.get_feature_num({'height':preproc.im_height, 'width':preproc.im_width})\n",
    "dec = RL_components['Decoder'](decoder_opts)\n",
    "rlnet = RL_components['RLNet'](enc, dec)\n",
    "policy = RL_components['Policy'](rlnet)\n",
    "trainer = RL_components['Trainer'](rlnet, mem, trainer_opts)\n",
    "keeper = RL_components['Keeper']([enc, dec, policy, mem], keeper_opts)  # objects has \"save/load\" interface\n",
    "alg = RLAlgorithm(keeper, env, preproc, mem, policy, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framework-DiffPreproc1-END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dplay_utils.xdeploy as xd\n",
    "xd.deploy(THIS_NOTEBOOK, 'DiffPreproc1', RL_components, '../../RUNS/DiffPreproc1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
